{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
    "\n",
    "\n",
    "# Week 3: Embedding-Based Retrieval\n",
    "\n",
    "### What we are building\n",
    "The goal of Embedding-Based Retrieval is to retrieve top-k candidates given a query based on embedding similarity/distance. A common application for this is given a query/sentence/document, find top-k similar candidates wrt query. While this is usually solved using TF-IDF/Information Retrieval (IR) based approaches, it is becoming more and more common in the industry to use an embedding based approach: encode the query and document as an embedding and use approximate nearest neighbor search to find top-k candidates in real-time.\n",
    "\n",
    "We will build a system to find duplicate questions on Quora using a [dataset released by Quora](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs). A very common problem for forums/QA websites is trying to determine whether a question has already been asked before a user posts it.\n",
    "\n",
    "We will continue to apply our learning philosophy of repetition as we build multiple models of increasing complexity in the following order:\n",
    "\n",
    "1. Retrieval based on WordVectors\n",
    "1. Using BERT\n",
    "1. Using Sentence BERT\n",
    "1. Using Cohere Sentence Embeddings\n",
    "\n",
    "###  Evaluation\n",
    "We will evaluate our models along the following metrics:\n",
    "\n",
    "1. Recall@k: the proportion of relevant items found in the top-k matches\n",
    "1. Mean Reciprocal Rank: the rank of the first relevant item with respect to the top-k.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. We have provided scaffolding for all the boilerplate FAISS code to get to our baseline model. This covers downloading and parsing the dataset, and training code for the baseline model. **Make sure to read all the steps and internalize what is happening**.\n",
    "1. At this point in our model, we will aim to use BERT embeddings. **Does this improve accuracy?**\n",
    "1. In the third model, we will use Sentence BERT and then we'll see if they can boost up our model. **How do you think this model will perform?**\n",
    "1. **Extension**: We have suggested a bunch of extensions to the project so go crazy! Tweak any parts of the pipeline, and see if you can beat all the current models.\n",
    "\n",
    "### Code Overview\n",
    "\n",
    "- Dependencies: Install and import python dependencies\n",
    "- Project\n",
    "  - Dataset: Download the Quora dataset\n",
    "  - Indexer: Function to manage and create a Faiss Index\n",
    "  - Model 1: Word Vectors\n",
    "  - Model 2: BERT\n",
    "  - Model 3: Sentence BERT\n",
    "  - Model 4: Cohere Sentence Embeddings\n",
    "- Extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "‚ú® Now let's get started! To kick things off, as always, we will install some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the relevant libraries\n",
    "import csv\n",
    "import spacy\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast, DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Load the spacy data\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Based Retrieval\n",
    "\n",
    "‚ú® Let's Begin ‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Processing (Common to ALL Solutions)\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "Download the duplicate questions [dataset released by Quora](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget 'http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv'\n",
    "!mkdir qqp\n",
    "!mv quora_duplicate_questions.tsv qqp/\n",
    "!ls qqp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Now we see all of our files. Let's poke at one of them before we start parsing our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate']\n",
      "['0', '1', '2', 'What is the step by step guide to invest in share market in india?', 'What is the step by step guide to invest in share market?', '0']\n",
      "['1', '3', '4', 'What is the story of Kohinoor (Koh-i-Noor) Diamond?', 'What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?', '0']\n",
      "['2', '5', '6', 'How can I increase the speed of my internet connection while using a VPN?', 'How can Internet speed be increased by hacking through DNS?', '0']\n",
      "['3', '7', '8', 'Why am I mentally very lonely? How can I solve it?', 'Find the remainder when [math]23^{24}[/math] is divided by 24,23?', '0']\n",
      "['4', '9', '10', 'Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?', 'Which fish would survive in salt water?', '0']\n",
      "['5', '11', '12', 'Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?', \"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\", '1']\n",
      "['6', '13', '14', 'Should I buy tiago?', 'What keeps childern active and far from phone and video games?', '0']\n",
      "['7', '15', '16', 'How can I be a good geologist?', 'What should I do to be a great geologist?', '1']\n",
      "['8', '17', '18', 'When do you use „Ç∑ instead of „Åó?', 'When do you use \"&\" instead of \"and\"?', '0']\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE = \"qqp/quora_duplicate_questions.tsv\"\n",
    "\n",
    "# The file is a 6-column tab separated file.\n",
    "# The first column is the row_id, second and third questions are ids of\n",
    "# specific questions, followed by the text of questions.\n",
    "# The last column captures if the two questions are duplicates\n",
    "with open(DATA_FILE, 'r', newline='\\n') as file:\n",
    "  reader = csv.reader(file, delimiter = '\\t')\n",
    "  # Read first 10 lines\n",
    "  for i in range(10):\n",
    "    print(next(reader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has more than 500k questions! We are going to parse the full dataset and create a sample of 10k questions to experiment with in our models since BERT training & inference can be really slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique questions: 10000\n",
      "Number of question with duplicates: 3810\n",
      "Number of questions in sample: 10000\n",
      "Number of duplicate pairs in sample: 3589\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Util function to parse the file\n",
    "\"\"\"\n",
    "def parse_sample_dataset(file_path, sample_max_id):\n",
    "  \"\"\"\n",
    "  Inputs:\n",
    "    file_path: Path to the raw data file\n",
    "    sample_max_id: Max question id to be considered in the sampled dataset\n",
    "\n",
    "  Returns 4 objects:\n",
    "    1. QuestionMap: list of all question ids\n",
    "    2. DuplicatesMap: Map of questionID to it's duplicates\n",
    "    3. SampleDataset: list of questionIds in the sample\n",
    "    4. SampleEvalDataset: list of pair of duplicate questions in the sample\n",
    "  \"\"\"\n",
    "  question_map = {}\n",
    "  duplicates_map = defaultdict(set)\n",
    "  sample_dataset = set([])\n",
    "  sample_eval_dataset = []\n",
    "\n",
    "  with open(file_path, 'r', newline='\\n') as file:\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "    next(reader)  # Skip the header line\n",
    "\n",
    "    for row in reader:\n",
    "      if len(row) != 6: # Skip incomplete rows\n",
    "        continue\n",
    "\n",
    "      # Limit the sample size of the dataset at max_id\n",
    "      # Make sure all 4 objects start at index 0\n",
    "      qid1, qid2, label = int(row[1]) - 1, int(row[2]) - 1, int(row[5])\n",
    "      if qid1 < sample_max_id and qid2 < sample_max_id:\n",
    "\n",
    "        if qid1 not in question_map:\n",
    "          question_map[qid1] = str(row[3])\n",
    "        if qid2 not in question_map:\n",
    "          question_map[qid2] = str(row[4])\n",
    "\n",
    "        if label == 1:\n",
    "          duplicates_map[qid1].add(qid2)\n",
    "          duplicates_map[qid2].add(qid1)\n",
    "\n",
    "          sample_eval_dataset.append((qid1, qid2))\n",
    "\n",
    "        sample_dataset.add(qid1)\n",
    "        sample_dataset.add(qid2)\n",
    "\n",
    "  # sample dataset duplicates removed via set(), so turn back into list\n",
    "  return question_map, duplicates_map, list(sample_dataset), sample_eval_dataset\n",
    "\n",
    "question_map, duplicates_map, sample_dataset, sample_eval_dataset, = parse_sample_dataset(DATA_FILE, 10000)\n",
    "\n",
    "# Complete file: 537k unique questions, 400k duplicate.\n",
    "# To keep training time manageable limited to 10.000 (sample_max_id)\n",
    "print(\"Number of unique questions:\", len(question_map)) # 10.000\n",
    "print(\"Number of question with duplicates:\", len(duplicates_map)) # ~3.8k\n",
    "print(\"Number of questions in sample:\", len(sample_dataset)) # 10.000\n",
    "print(\"Number of duplicate pairs in sample:\", len(sample_eval_dataset)) # ~3.6k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval using Faiss -- TO BE COMPLETED\n",
    "\n",
    "You are now going to create an Indexer class that implements multiple functions for indexing, searching, and evaluating our retrieval model. FAISS documentation can be found in the wiki here: https://github.com/facebookresearch/faiss/wiki/Getting-started\n",
    "\n",
    "Some helpful FAISS guides are:\n",
    "- https://www.pinecone.io/learn/faiss-tutorial/\n",
    "- https://www.pinecone.io/learn/vector-indexes/\n",
    "\n",
    "You need to implement the following functions:\n",
    "\n",
    "1. **search**: Implement a function that takes a question and top_k variable and returns either the matched strings or the ids to the user as a\n",
    "    1. Call the search API on the faiss_index to look up similar sentences using `faiss_index.search`\n",
    "    2. Parse the output to either return [sentence_id, score] tuples or [sentence, score] tuples based on the input parameter\n",
    "    3. Sort the output by the score in descending order\n",
    "\n",
    "1. **evaluate**: Sample num_docs pairs from the evaluation dataset and then check if the qid2 is present in the top-k results\n",
    "    1. For each eval sample, find the top_k matches for the qid1\n",
    "    2. See if the qid2 is in one of the matches\n",
    "    3. If yes, append (1) to the recall array otherwise append (0)\n",
    "    4. Implement MRR (Mean reciprocal rank) addition based on the position of qid2 in matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaissIndexer:\n",
    "  def __init__(self, dataset,\n",
    "               question_map,\n",
    "               eval_dataset,\n",
    "               batch_size,\n",
    "               sentence_vector_dim,\n",
    "               vectorizer):\n",
    "    self.question_map = question_map\n",
    "    self.dataset = dataset\n",
    "    self.eval_dataset = eval_dataset\n",
    "    self.batch_size = batch_size\n",
    "    self.vectorizer = vectorizer\n",
    "    # FlatIP uses inner product\n",
    "    self.faiss_index = faiss.IndexFlatIP(sentence_vector_dim)\n",
    "\n",
    "\n",
    "  def split_list(self, lst: list, sublist_size: int):\n",
    "    sublists = []\n",
    "    # Split list into even chunks/sublists/batches\n",
    "    for i in range(0, len(lst), sublist_size):\n",
    "      sublists.append(lst[i:i + sublist_size])\n",
    "    return sublists\n",
    "\n",
    "\n",
    "  def index(self):\n",
    "    sentence_vectors = []\n",
    "\n",
    "    print(\"Start indexing!\")\n",
    "    for sentence_ids in tqdm(self.split_list(self.dataset, self.batch_size)):\n",
    "      # Retrieve sentences based on qid\n",
    "      sentences = [question_map[qid] for qid in sentence_ids]\n",
    "      # Get embeddings of the sentences (Spacy, ..., OpenAI, Cohere)\n",
    "      sentence_vectors_batch = self.vectorizer.vectorize(sentences)\n",
    "      # Add batch to temporary list\n",
    "      sentence_vectors.append(sentence_vectors_batch)\n",
    "\n",
    "    # Add all batches from temporary list to index\n",
    "    self.faiss_index.add(np.array(np.concatenate(sentence_vectors, axis=0)))\n",
    "    print(\"\\nDone indexing!\")\n",
    "\n",
    "\n",
    "  def search(self, question: str, top_k: int, return_ids=False):\n",
    "    \"\"\"Given any sentence (typed by the user)\n",
    "    We return a list of top_k(sentence, sim_score) or top_k(sentence_ids, sim_score)\n",
    "\n",
    "    NOTE: The output type is controlled by the return_ids flag\n",
    "\n",
    "    1. Call the search API on the faiss_index to look up similar sentences\n",
    "       using `faiss_index.search`\n",
    "    2. Parse the output to either return [sentence_id, score] tuples or\n",
    "       [sentence, score] tuples based on return_ids being true/false\n",
    "    3. Sort the output by the score in descending order\n",
    "    \"\"\"\n",
    "\n",
    "    # NOTE: We converted the question to a list here to match the signature\n",
    "    # of the vectorize function\n",
    "    question_vectors = self.vectorizer.vectorize([question])\n",
    "\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    scores, indices = self.faiss_index.search(question_vectors, top_k)\n",
    "    if return_ids:\n",
    "      return list(zip(indices[0], scores[0]))\n",
    "    else:\n",
    "      return list(zip([self.question_map[index] for index in indices[0]], scores[0]))\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "\n",
    "    # Output is a List[(qid, score), (qid, score), (qid, score)] or\n",
    "    # List[(q, score), (q, score), (q, score)] based on return_ids\n",
    "    # Output is sorted in descending order of score\n",
    "    return output\n",
    "\n",
    "\n",
    "  def evaluate(self, top_k: int, eval_sample_size: int):\n",
    "    \"\"\"Sample num_docs pairs from the evaluation dataset and then check\n",
    "    if the qid2 is present in the top-k results\n",
    "\n",
    "    1. For each eval sample, find the top_k matches for the qid1\n",
    "    2. See if the qid2 is in one of the matches\n",
    "    3. If yes, append (1) to the recall array otherwise append (0)\n",
    "    4. Implement MRR (Mean reciprocal rank) addition based on the position of qid2 in matches\n",
    "      - Note: MRR is equivalent to mean([1/r or 0 for each sample])\n",
    "    \"\"\"\n",
    "    # Sample from evaluation dataset as proxy for performance metrics\n",
    "    eval_sample = random.sample(self.eval_dataset, eval_sample_size)\n",
    "\n",
    "    # Retrieval metrics which only care about if searched for\n",
    "    # item is present among the results.\n",
    "    recall_at_k = [] # Relevant items vs total of relevant items\n",
    "    mean_reciprocal_rank = [] # Rank of the first relevant item\n",
    "\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    # 1. For each eval sample, find the top_k matches for the qid1\n",
    "    for qid1, qid2 in tqdm(eval_sample, desc=\"Evaluating random samples\"):\n",
    "      matches = self.search(self.question_map[qid1], top_k, return_ids=True)\n",
    "      matches = [qid for qid, _ in matches]\n",
    "      # 2. See if the qid2 is in one of the matches\n",
    "      # 3. If yes, append (1) to the recall array otherwise append (0)\n",
    "      if qid2 in matches:\n",
    "        recall_at_k.append(1)\n",
    "        # 4. Implement MRR (Mean reciprocal rank) addition based on the position of qid2 in matches\n",
    "        mean_reciprocal_rank.append(1 / (matches.index(qid2) + 1))\n",
    "      else:\n",
    "        recall_at_k.append(0)\n",
    "        mean_reciprocal_rank.append(0)\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "\n",
    "    print(\"\\nRecall@{}:\\t\\t{:0.2f}%\".format(top_k, np.mean(np.array(recall_at_k) * 100.0)))\n",
    "    print(\"Mean Reciprocal Rank:\\t{:0.2f}\".format(np.mean(np.array(mean_reciprocal_rank))))\n",
    "\n",
    "\n",
    "  # Helper function to train, search and evaluate similar output from all the models created.\n",
    "  def train_and_evaluate(self,\n",
    "                         question_example: str,\n",
    "                         top_k: int = 10,\n",
    "                         eval_sample_size: int = 1000\n",
    "                         ):\n",
    "    print(\"---- Indexing ----\")\n",
    "    self.index()\n",
    "    print(\"\\n---- Search ----\")\n",
    "    results = self.search(question_example, top_k, return_ids=False)\n",
    "    print(\"Questions similar to:\", question_example)\n",
    "    for i, (q, s) in enumerate(results):\n",
    "      print(f\"{i} Question: {q} with score {s}\")\n",
    "    print(\"\\n---- Evaluation ----\")\n",
    "    self.evaluate(top_k, eval_sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Model Test\n",
    "\n",
    "Really small sample of 4 sentences to make sure we can test our implementation of the FAISS search function correctly. We just project the 4 questions in a 2-d space where they are placed on the X-Axis if the word `invest` is present and on the Y-axis if `kohinoor` is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions:\n",
      "0 : What is the step by step guide to invest in share market in india?\n",
      "1 : What is the step by step guide to invest in share market?\n",
      "2 : What is the story of Kohinoor (Koh-i-Noor) Diamond?\n",
      "3 : What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\n"
     ]
    }
   ],
   "source": [
    "dummy_ids = sample_dataset[:4]\n",
    "print(\"Questions:\")\n",
    "for i in dummy_ids:\n",
    "  print(i, \":\", question_map[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13662.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done indexing!\n",
      "Questions similar to: invest\n",
      "0 Question: What is the step by step guide to invest in share market? with score 0.6792926788330078\n",
      "1 Question: What is the step by step guide to invest in share market in india? with score 0.3259030282497406\n",
      "2 Question: What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back? with score 0.0\n",
      "3 Question: What is the story of Kohinoor (Koh-i-Noor) Diamond? with score 0.0\n",
      "\n",
      "Questions similar to: Kohinoor\n",
      "0 Question: What is the story of Kohinoor (Koh-i-Noor) Diamond? with score 0.1971409022808075\n",
      "1 Question: What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back? with score 0.009744079783558846\n",
      "2 Question: What is the step by step guide to invest in share market? with score 0.0\n",
      "3 Question: What is the step by step guide to invest in share market in india? with score 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class DummyVectorizer:\n",
    "  def __init__(self, sentence_vector_dim):\n",
    "    self.sentence_vector_dim = sentence_vector_dim\n",
    "\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences.\n",
    "\n",
    "    1. Tokenize each sentence and create vectors for each token in the sentence\n",
    "    2. Sentence vector is the mean of word vectors of each token\n",
    "    3. Stack the sentence vectors into a numpy array using np.stack\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for sentence in sentences:\n",
    "      if \"invest\" in sentence:\n",
    "        # If \"invest\" is present place it on the X-Axis\n",
    "        vectors.append(np.array([random.random(), 0], dtype=np.float32))\n",
    "      elif \"Kohinoor\" in sentence:\n",
    "        # If \"Kohinoor\" is present place it on the Y-Axis\n",
    "        vectors.append(np.array([0, random.random()], dtype=np.float32))\n",
    "    return np.stack(vectors)\n",
    "\n",
    "\n",
    "di = FaissIndexer(dummy_ids,\n",
    "                  question_map,\n",
    "                  sample_eval_dataset,\n",
    "                  batch_size=1024,\n",
    "                  sentence_vector_dim=2,\n",
    "                  vectorizer=DummyVectorizer(2)\n",
    "                  )\n",
    "\n",
    "di.index()\n",
    "\n",
    "results = di.search(\"invest\", 4)\n",
    "print(\"Questions similar to:\", \"invest\")\n",
    "for i, (q, s) in enumerate(results):\n",
    "  print(f\"{i} Question: {q} with score {s}\")\n",
    "\n",
    "results = di.search(\"Kohinoor\", 4)\n",
    "print(\"\\nQuestions similar to:\", \"Kohinoor\")\n",
    "for i, (q, s) in enumerate(results):\n",
    "  print(f\"{i} Question: {q} with score {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "You may be wondering, \"When are we going to start building models?\" And, the answer is NOW! Finally the time has come to build our baseline model, and then we'll work towards improving it.\n",
    "\n",
    "\n",
    "**NOTE**: We will be using the sample dataset since BERT is really slow and processing the full dataset will take a lot of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Averaging Word Vectors --- TO BE COMPLETED\n",
    "##### <font color='red'>Expected recall@10: ~5%, MRR: ~0.02</font>\n",
    "\n",
    "Complete the `vectorize` function using Spacy provided word embeddings. This is something we've done already :)\n",
    "\n",
    "Implementation:\n",
    "\n",
    "1. Tokenize each sentence and get wordVectors for each token in the sentence using Spacy\n",
    "2. Sentence vector is the mean of word vectors of each token\n",
    "3. Stack the sentence vectors into a numpy array using np.stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Indexing ----\n",
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 25.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done indexing!\n",
      "\n",
      "---- Search ----\n",
      "Questions similar to: how can i invest in stock market in india?\n",
      "0 Question: In how many ways can we create object in Java? with score 1264.394287109375\n",
      "1 Question: How can we find happiness in life? with score 1203.2908935546875\n",
      "2 Question: I want to connect with you, how can I do that? with score 1198.193359375\n",
      "3 Question: How can I can concentrate well in studies? with score 1197.028564453125\n",
      "4 Question: what can i do to become fair? with score 1191.52001953125\n",
      "5 Question: Why do we need to study? with score 1189.1204833984375\n",
      "6 Question: Minimum marks in NEET2017 to get admission in IISC? with score 1185.0025634765625\n",
      "7 Question: How can we earn money online in india? with score 1163.95947265625\n",
      "8 Question: How can I be happy if I don't have any reason to be? with score 1163.731201171875\n",
      "9 Question: What to do when you don't want to do? with score 1161.207275390625\n",
      "\n",
      "---- Evaluation ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating random samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 2665.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall@10:\t\t5.00%\n",
      "Mean Reciprocal Rank:\t0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class SpacyVectorizer:\n",
    "  def __init__(self, sentence_vector_dim):\n",
    "    self.sentence_vector_dim = sentence_vector_dim\n",
    "\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences.\n",
    "\n",
    "    1. Tokenize each sentence and create vectors for each token in the sentence\n",
    "    2. Sentence vector is the mean of word vectors of each token\n",
    "    3. Stack the sentence vectors into a numpy array using np.stack\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for sentence in sentences:\n",
    "\n",
    "      ### TO BE COMPLETED ###\n",
    "      sentence_vector = np.mean([token.vector for token in nlp.make_doc(sentence) if token.has_vector], axis=0)\n",
    "      ### TO BE COMPLETED ###\n",
    "\n",
    "      vectors.append(sentence_vector)\n",
    "    return np.stack(vectors)\n",
    "\n",
    "spacyIndex = FaissIndexer(sample_dataset,\n",
    "                  question_map,\n",
    "                  sample_eval_dataset,\n",
    "                  batch_size=1024,\n",
    "                  sentence_vector_dim=300,\n",
    "                  vectorizer=SpacyVectorizer(300))\n",
    "\n",
    "spacyIndex.train_and_evaluate(question_example = \"how can i invest in stock market in india?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: BERT Embeddings --- TO BE COMPLETED\n",
    "##### <font color='red'>Expected recall@10: ~48%, MRR: ~0.19</font>\n",
    "\n",
    "Compute the sentence embeddings using the BERT model and complete the `vectorize` function. Feel free to reference any documentation from https://huggingface.co/.\n",
    "\n",
    "\n",
    "Implementation:\n",
    "\n",
    "1. Tokenize batch of sentences using `self.tokenizer`\n",
    "2. Pipe the inputs through the BERT model to create the output logits\n",
    "3. Normalize the batch output\n",
    "\n",
    "**NOTE: This model is really slow and will take a while to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Indexing ----\n",
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [01:05<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done indexing!\n",
      "\n",
      "---- Search ----\n",
      "Questions similar to: how can i invest in stock market in india?\n",
      "0 Question: I wish to start investing in Equity and Mutual Funds. Where should I open Demat account for best rates, transaction charges and so on? I am NRI. with score 0.8770731091499329\n",
      "1 Question: What is the step by step guide to invest in share market in india? with score 0.874489426612854\n",
      "2 Question: What are mutual funds and which is the best one in India in which to invest? with score 0.8723899126052856\n",
      "3 Question: What will be the effect of banning 500 and 1000 notes on stock markets in India? with score 0.8636163473129272\n",
      "4 Question: What will be the effect of banning 500 and 1000 Rs notes on real estate sector in India? Can we expect sharp fall in prices in short/long term? with score 0.861491322517395\n",
      "5 Question: What are your views on Modi governments decision to demonetize 500 and 1000 rupee notes? How will this affect economy? with score 0.8532259464263916\n",
      "6 Question: What is the best time to invest in real estate market after demonetisation? with score 0.8521273732185364\n",
      "7 Question: Do you think India will be able to curb blank money? with score 0.8468542098999023\n",
      "8 Question: What should I do to make money online in India? with score 0.8458528518676758\n",
      "9 Question: How much does an Ola Mini cab earn on average per month from an investor's point of view in Mumbai? with score 0.8440940976142883\n",
      "\n",
      "---- Evaluation ----\n",
      "\n",
      "Recall@10:\t\t46.10%\n",
      "Mean Reciprocal Rank:\t0.18\n"
     ]
    }
   ],
   "source": [
    "class BertVectorizer:\n",
    "  def __init__(self):\n",
    "    self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    self.model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences.\n",
    "\n",
    "    1. Tokenize batch of sentences using `self.tokenizer`\n",
    "    2. Pipe the inputs through the BERT model to create the output logits\n",
    "    3. Normalize the batch output\n",
    "    \"\"\"\n",
    "\n",
    "    ### TO BE COMPLETED ###\n",
    "    inputs = self.tokenizer(sentences, return_tensors='pt', padding=True)\n",
    "    outputs = self.model(**inputs)\n",
    "    model_output = outputs.last_hidden_state\n",
    "    ### TO BE COMPLETED ###\n",
    "\n",
    "    return F.normalize(torch.mean(model_output, dim=1), dim=1).detach().numpy()\n",
    "\n",
    "\n",
    "bertIndex = FaissIndexer(sample_dataset,\n",
    "                  question_map,\n",
    "                  sample_eval_dataset,\n",
    "                  batch_size=32,\n",
    "                  sentence_vector_dim=768,\n",
    "                  vectorizer=BertVectorizer())\n",
    "\n",
    "bertIndex.train_and_evaluate(question_example = \"how can i invest in stock market in india?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Sentence Transformer --- TO BE COMPLETED\n",
    "##### <font color='red'>Expected recall@10: ~92%, MRR: ~0.34</font>\n",
    "\n",
    "Compute the sentence embeddings using the Sentence BERT model and complete the `vectorize` function. Feel free to look up documentation on https://www.sbert.net/.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "1. Pipe the input sentences through the Sentence BERT model to create the output logits\n",
    "2. Normalize the batch output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9295db67a9f4b09baae9cae66095259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661bc35228c44da0997e51f1bae2d913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6159e74477b046a99d621ed34cb8e096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f891384233ba447ca4f5094e97b34c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62436ed5ae2342f8b5a089d64b588a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c191feb25b44e79b79a2bbb9e4fa20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a0f2902f1a46e189cb9defcc983d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d645f54b19641588fefc1080307fe2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fbfb425aad4a0b860aa072b75863ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04ce3e198ae4d16ae5a0424446735a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c897e9e3b3ad4df6b8dca9c93cabfbd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Indexing ----\n",
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:18<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done indexing!\n",
      "\n",
      "---- Search ----\n",
      "Questions similar to: how can i invest in stock market in india?\n",
      "0 Question: What is the step by step guide to invest in share market in india? with score 0.733176589012146\n",
      "1 Question: I am 17 and I want to invest money in stock market where should I start? with score 0.6957337260246277\n",
      "2 Question: What are the ways to learn about stock market? with score 0.6243616342544556\n",
      "3 Question: How do I start investing in shares or stocks? What is the minimum requirement? with score 0.6239825487136841\n",
      "4 Question: What is the best way to learn about stock market? with score 0.6222878098487854\n",
      "5 Question: What is the step by step guide to invest in share market? with score 0.6042823195457458\n",
      "6 Question: What is the best way to learn about investing in the stock market and what stocks to buy? with score 0.6032655835151672\n",
      "7 Question: What is the best way to learn about stock markets? with score 0.5846708416938782\n",
      "8 Question: How do I buy stocks? with score 0.5778073072433472\n",
      "9 Question: What are mutual funds and which is the best one in India in which to invest? with score 0.5576667785644531\n",
      "\n",
      "---- Evaluation ----\n",
      "\n",
      "Recall@10:\t\t92.30%\n",
      "Mean Reciprocal Rank:\t0.34\n"
     ]
    }
   ],
   "source": [
    "class SentenceBertVectorizer:\n",
    "  def __init__(self):\n",
    "    self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences.\n",
    "\n",
    "    1. Pipe the input sentences through the Sentence BERT model to create the output logits\n",
    "    2. Normalize the batch output\n",
    "    \"\"\"\n",
    "\n",
    "    ### TO BE COMPLETED ###\n",
    "    sentence_vectors = self.model.encode(sentences)\n",
    "    ### TO BE COMPLETED ###\n",
    "\n",
    "    return sentence_vectors / np.expand_dims(np.linalg.norm(sentence_vectors, axis=1), axis=1)\n",
    "\n",
    "\n",
    "SBertIndex = FaissIndexer(sample_dataset,\n",
    "                  question_map,\n",
    "                  sample_eval_dataset,\n",
    "                  batch_size=1024,\n",
    "                  sentence_vector_dim=384,\n",
    "                  vectorizer=SentenceBertVectorizer())\n",
    "\n",
    "SBertIndex.train_and_evaluate(question_example = \"how can i invest in stock market in india?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL\n",
    "**This section requires a paid account with OpenAI.  It is completely optional and can be skipped.**\n",
    "### Model 4: OpenAI Text Embeddings\n",
    "##### <font color='red'>Expected recall@10: ~92%, MRR: ~0.32</font>\n",
    "\n",
    "Make sure create an OpenAI account and make an API key.\n",
    "Compute the sentence embeddings using the OpenAI API and complete the `vectorize` function. Feel free to look up documentation on https://platform.openai.com/docs/api-reference/embeddings.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "1. Pipe the input sentences through the OpenAI API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Indexing ----\n",
      "Start indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:28<00:00,  5.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done indexing!\n",
      "\n",
      "---- Search ----\n",
      "Questions similar to: how can i invest in stock market in india?\n",
      "0 Question: What is the step by step guide to invest in share market in india? with score 0.7989835739135742\n",
      "1 Question: What is the step by step guide to invest in share market? with score 0.6763179898262024\n",
      "2 Question: How do I buy stocks? with score 0.6537752151489258\n",
      "3 Question: I am 17 and I want to invest money in stock market where should I start? with score 0.6437757611274719\n",
      "4 Question: How can I make money online in India? with score 0.6310142278671265\n",
      "5 Question: How can we earn money online in india? with score 0.6182802319526672\n",
      "6 Question: What should I do to make money online in India? with score 0.6099871397018433\n",
      "7 Question: What are the ways to learn about stock market? with score 0.6030036211013794\n",
      "8 Question: What is the best way to learn about stock market? with score 0.5974246263504028\n",
      "9 Question: What are mutual funds and which is the best one in India in which to invest? with score 0.5897570848464966\n",
      "\n",
      "---- Evaluation ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating random samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:49<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall@10:\t\t92.00%\n",
      "Mean Reciprocal Rank:\t0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.Client(api_key=\"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\")\n",
    "\n",
    "embeddings_model = \"text-embedding-3-small\"\n",
    "\n",
    "class OpenAIVectorizer:\n",
    "  def vectorize(self, sentences):\n",
    "    \"\"\"Return sentence vectors for the batch of sentences.\n",
    "\n",
    "    See the OpenAI API documentation for more details: https://platform.openai.com/docs/guides/embeddings/use-cases\n",
    "    \"\"\"\n",
    "\n",
    "    ### TO BE COMPLETED ####\n",
    "    response = client.embeddings.create(\n",
    "        input=sentences,\n",
    "        model=embeddings_model\n",
    "    )\n",
    "\n",
    "    sentence_vectors = [data.embedding for data in response.data]\n",
    "    ### TO BE COMPLETED ###\n",
    "\n",
    "    # Convert from float64 to float32 to prevent bug:\n",
    "    # https://github.com/facebookresearch/faiss/issues/461\n",
    "\n",
    "    return np.float32(np.array(sentence_vectors) / np.expand_dims(np.linalg.norm(sentence_vectors, axis=1), axis=1))\n",
    "\n",
    "openaiIndex = FaissIndexer(sample_dataset,\n",
    "                  question_map,\n",
    "                  sample_eval_dataset,\n",
    "                  batch_size=2048,\n",
    "                  sentence_vector_dim=1536, # This is the length of the OpenAI embeddings model \"text-embedding-3-small\"\n",
    "                  vectorizer=OpenAIVectorizer())\n",
    "\n",
    "openaiIndex.train_and_evaluate(question_example = \"how can i invest in stock market in india?\",\n",
    "                               eval_sample_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ CONGRATULATIONS on finishing the assignment!!! We built a real model with an actual datasets for a problem that is used every time a new Quora question gets created!!\n",
    "\n",
    "As for why did SentenceBERT & Cohere perform so well, we'll cover that in Siamese networks in week4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "\n",
    "Now that you've worked through the project there is a lot more for us to try:\n",
    "\n",
    "- Try out `SentenceBert` and `SpacyVectors` on the entire dataset rather the sample and see what you get?\n",
    "- Try different transformer models from hugging face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
