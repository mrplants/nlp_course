{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Tokenization\n",
    "\n",
    "### What we are building\n",
    "Tokenization is the task of chopping text into pieces, called tokens. As you might have observed going through prior notebooks, vectorization and tokenization have a huge influence on the output of the exact same model.\n",
    "\n",
    "We will compare the different tokenizers for different sizes of vocabulary on Botchan, a novel written by Natsume Sōseki in 1906. We'll see what percentage of vocabulary would be considered OOV (out-of-vocab) at different sizes.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. We have provide scaffolding for all the different tokenizers and added an assert to make sure the output is same as expected.\n",
    "1. Most of the tokenizers are already somethings you've seen before, but we'll dive deeper into SentencePiece.\n",
    "\n",
    "### Code Overview\n",
    "\n",
    "- Dependencies: Install and import python dependencies\n",
    "- Tokenizers\n",
    "  - WhitespaceTokenizer\n",
    "  - CharacterTokenizer\n",
    "  - SpacyTokenizer\n",
    "  - BERTTokenizer\n",
    "  - SentencePieceTokenizer\n",
    "- Extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "✨ Now let's get started! To kick things off, as always, we will install some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the relevant libraries\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "import sentencepiece as spm\n",
    "\n",
    "from collections import defaultdict\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Download the Botchan novel from [SentencePiece repository](https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-16 13:18:37--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 278779 (272K) [text/plain]\n",
      "Saving to: ‘botchan.txt’\n",
      "\n",
      "botchan.txt         100%[===================>] 272.25K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2024-02-16 13:18:37 (15.5 MB/s) - ‘botchan.txt’ saved [278779/278779]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
    "\n",
    "lines = []\n",
    "with open('botchan.txt', 'r') as the_file:\n",
    "  lines = the_file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants for sample sentence and thresholds we'll be using throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SENTENCE = \"I'm learning NLP. Aren't my projects awesome?\"\n",
    "THRESHOLDS = [1000, 2000, 3000, 4000, 5000, 7500, 10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Tokenizer class that implements the coverage report function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "  def __init__(self, lines):\n",
    "    self.vocab_size = 0\n",
    "    token_dict = defaultdict(int)\n",
    "\n",
    "    for line in lines:\n",
    "      for token in self.tokenize(line):\n",
    "        self.vocab_size += 1\n",
    "        token_dict[token] += 1\n",
    "\n",
    "    self.token_counts = sorted(token_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  def tokenize(self, sentence):\n",
    "    raise ValueError(\"TO BE IMPLEMENTED\")\n",
    "\n",
    "  def coverage(self, threshold):\n",
    "    return sum([x[1] for x in self.token_counts[:threshold]]) / self.vocab_size\n",
    "\n",
    "  def coverage_report(self, thresholds):\n",
    "    # For each threshold print the percentage of coverage and OOV\n",
    "    for tv in thresholds:\n",
    "      coverage = self.coverage(tv) * 100\n",
    "      print(\"For vocab size: %d, coverage is: %.2f%% and oov is: %.2f%%\" % (tv, coverage, 100-coverage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Part:1 - Whitespace based separators\n",
    "##### <font color='red'>Expected:  for vocab size 1,000 -> coverage: 72.79%</font>\n",
    "##### <font color='red'>Expected:  for vocab size 10,000 -> coverage: 99.43%</font>\n",
    "\n",
    "Tokenizer that splits the string sentences into tokens using  whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vocab size: 1000, coverage is: 72.79% and oov is: 27.21%\n",
      "For vocab size: 2000, coverage is: 80.11% and oov is: 19.89%\n",
      "For vocab size: 3000, coverage is: 84.41% and oov is: 15.59%\n",
      "For vocab size: 4000, coverage is: 87.66% and oov is: 12.34%\n",
      "For vocab size: 5000, coverage is: 89.62% and oov is: 10.38%\n",
      "For vocab size: 7500, coverage is: 94.52% and oov is: 5.48%\n",
      "For vocab size: 10000, coverage is: 99.43% and oov is: 0.57%\n"
     ]
    }
   ],
   "source": [
    "class WhiteSpaceTokenizer(Tokenizer):\n",
    "  def tokenize(self, sentence):\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    output = sentence.split(' ')\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    \n",
    "    return output\n",
    "\n",
    "white_space_tokenizer = WhiteSpaceTokenizer(lines)\n",
    "assert white_space_tokenizer.tokenize(SAMPLE_SENTENCE) == [\"I'm\", 'learning', 'NLP.', \"Aren't\", 'my', 'projects', 'awesome?']\n",
    "white_space_tokenizer.coverage_report(THRESHOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Part:2 - Character based tokenizer\n",
    "##### <font color='red'>Expected:  for vocab size 1,000 -> coverage: 100.00%</font>\n",
    "##### <font color='red'>Expected:  for vocab size 10,000 -> coverage: 100.00%</font>\n",
    "\n",
    "Tokenizer that splits the string sentences into individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vocab size: 1000, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 2000, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 3000, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 4000, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 5000, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 7500, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 10000, coverage is: 100.00% and oov is: 0.00%\n"
     ]
    }
   ],
   "source": [
    "class CharacterTokenizer(Tokenizer):\n",
    "  def tokenize(self, sentence):\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    output = list(sentence)\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    \n",
    "    return output\n",
    "\n",
    "character_tokenizer = CharacterTokenizer(lines)\n",
    "assert character_tokenizer.tokenize(SAMPLE_SENTENCE) == ['I', \"'\", 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'N', 'L', 'P', '.', ' ', 'A', 'r', 'e', 'n', \"'\", 't', ' ', 'm', 'y', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', 's', ' ', 'a', 'w', 'e', 's', 'o', 'm', 'e', '?']\n",
    "character_tokenizer.coverage_report(THRESHOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Part:3 - Spacy Tokenizer\n",
    "##### <font color='red'>Expected:  for vocab size 1,000 -> coverage: 86%</font>\n",
    "##### <font color='red'>Expected:  for vocab size 10,000 -> coverage: 100%</font>\n",
    "\n",
    "Tokenizer that splits the string sentences into individual tokens using the Spacy's built in tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vocab size: 1000, coverage is: 86.00% and oov is: 14.00%\n",
      "For vocab size: 2000, coverage is: 91.94% and oov is: 8.06%\n",
      "For vocab size: 3000, coverage is: 95.08% and oov is: 4.92%\n",
      "For vocab size: 4000, coverage is: 96.67% and oov is: 3.33%\n",
      "For vocab size: 5000, coverage is: 98.21% and oov is: 1.79%\n",
      "For vocab size: 7500, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 10000, coverage is: 100.00% and oov is: 0.00%\n"
     ]
    }
   ],
   "source": [
    "class SpacyTokenizer(Tokenizer):\n",
    "  def tokenize(self, sentence):\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    output = [token.text for token in nlp.make_doc(sentence)]\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    \n",
    "    return output\n",
    "\n",
    "spacy_tokenizer = SpacyTokenizer(lines)\n",
    "assert spacy_tokenizer.tokenize(SAMPLE_SENTENCE) == ['I', \"'m\", 'learning', 'NLP', '.', 'Are', \"n't\", 'my', 'projects', 'awesome', '?']\n",
    "spacy_tokenizer.coverage_report(THRESHOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Part:4 - BERT Tokenizer\n",
    "##### <font color='red'>Expected:  for vocab size 1,000 -> coverage: 85.46%</font>\n",
    "##### <font color='red'>Expected:  for vocab size 10,000 -> coverage: 100.00%</font>\n",
    "\n",
    "BERT tokenizer provided by the hugging face library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vocab size: 1000, coverage is: 85.46% and oov is: 14.54%\n",
      "For vocab size: 2000, coverage is: 92.11% and oov is: 7.89%\n",
      "For vocab size: 3000, coverage is: 95.61% and oov is: 4.39%\n",
      "For vocab size: 4000, coverage is: 97.51% and oov is: 2.49%\n",
      "For vocab size: 5000, coverage is: 99.04% and oov is: 0.96%\n",
      "For vocab size: 7500, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 10000, coverage is: 100.00% and oov is: 0.00%\n"
     ]
    }
   ],
   "source": [
    "class BERTTokenizer(Tokenizer):\n",
    "  def __init__(self, tokenizer, lines):\n",
    "    self.tokenizer = tokenizer\n",
    "    super(BERTTokenizer, self).__init__(lines)\n",
    "\n",
    "  def tokenize(self, sentence):\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    output = self.tokenizer.tokenize(sentence)\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    \n",
    "    return output\n",
    "\n",
    "raw_bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer = BERTTokenizer(raw_bert_tokenizer, lines)\n",
    "assert bert_tokenizer.tokenize(SAMPLE_SENTENCE) == ['i', \"'\", 'm', 'learning', 'nl', '##p', '.', 'aren', \"'\", 't', 'my', 'projects', 'awesome', '?'], bert_tokenizer.tokenize(SAMPLE_SENTENCE)\n",
    "bert_tokenizer.coverage_report(THRESHOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Part:5 - SentencePieceTokenizer\n",
    "##### <font color='red'>Expected:  for vocab size 1,000 -> coverage: 99.88%</font>\n",
    "##### <font color='red'>Expected:  for vocab size 10,000 -> coverage: 100.00%</font>\n",
    "\n",
    "SentencePiece tokenizer works a bit differently from everything we've used so far. It needs to be trained with a vocabulary and a target size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4396 obj=8.63552 num_tokens=19717 num_tokens/piece=4.48521\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4389 obj=8.60658 num_tokens=19717 num_tokens/piece=4.49237\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m1000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71845 num_tokens=20449 num_tokens/piece=5.21259\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3921 obj=8.66278 num_tokens=20450 num_tokens/piece=5.21551\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2940 obj=8.96602 num_tokens=22797 num_tokens/piece=7.75408\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2940 obj=8.88624 num_tokens=22798 num_tokens/piece=7.75442\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2205 obj=9.2744 num_tokens=25530 num_tokens/piece=11.5782\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2205 obj=9.18615 num_tokens=25533 num_tokens/piece=11.5796\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1653 obj=9.6456 num_tokens=28789 num_tokens/piece=17.4162\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1653 obj=9.55867 num_tokens=28792 num_tokens/piece=17.418\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1239 obj=10.1208 num_tokens=32288 num_tokens/piece=26.0597\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1239 obj=10.0257 num_tokens=32288 num_tokens/piece=26.0597\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1100 obj=10.2354 num_tokens=33564 num_tokens/piece=30.5127\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1100 obj=10.2019 num_tokens=33564 num_tokens/piece=30.5127\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m1000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m1000.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m2000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71845 num_tokens=20449 num_tokens/piece=5.21259\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3921 obj=8.66278 num_tokens=20450 num_tokens/piece=5.21551\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2940 obj=8.96602 num_tokens=22797 num_tokens/piece=7.75408\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2940 obj=8.88624 num_tokens=22798 num_tokens/piece=7.75442\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2205 obj=9.2744 num_tokens=25530 num_tokens/piece=11.5782\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2205 obj=9.18615 num_tokens=25533 num_tokens/piece=11.5796\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=9.18811 num_tokens=25552 num_tokens/piece=11.6145\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=9.18747 num_tokens=25552 num_tokens/piece=11.6145\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m2000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m2000.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m3000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 3000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71845 num_tokens=20449 num_tokens/piece=5.21259\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3921 obj=8.66278 num_tokens=20450 num_tokens/piece=5.21551\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3300 obj=8.8047 num_tokens=21677 num_tokens/piece=6.56879\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3300 obj=8.76608 num_tokens=21678 num_tokens/piece=6.56909\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m3000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m3000.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m4000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4396 obj=8.63552 num_tokens=19717 num_tokens/piece=4.48521\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4389 obj=8.60658 num_tokens=19717 num_tokens/piece=4.49237\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m4000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m4000.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vocab size: 1000, coverage is: 99.88% and oov is: 0.12%\n",
      "For vocab size: 2000, coverage is: 99.85% and oov is: 0.15%\n",
      "For vocab size: 3000, coverage is: 99.84% and oov is: 0.16%\n",
      "For vocab size: 4000, coverage is: 99.83% and oov is: 0.17%\n",
      "For vocab size: 5000, coverage is: 99.83% and oov is: 0.17%\n",
      "For vocab size: 7500, coverage is: 100.00% and oov is: 0.00%\n",
      "For vocab size: 10000, coverage is: 100.00% and oov is: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m5000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m5000.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m5000.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m7500\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 7500\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m7500.model\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m10000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m10000.model\n"
     ]
    }
   ],
   "source": [
    "class SentencePieceTokenizer(Tokenizer):\n",
    "  def __init__(self, lines):\n",
    "    self.lines = lines\n",
    "    spm.SentencePieceTrainer.train(input='botchan.txt', model_prefix='m', vocab_size=4000)\n",
    "    self.sp = spm.SentencePieceProcessor()\n",
    "    self.sp.load('m.model')\n",
    "\n",
    "  def tokenize(self, sentence):\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    output = self.sp.encode_as_pieces(sentence)\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    \n",
    "    return output\n",
    "\n",
    "  def coverage(self, threshold):\n",
    "    # Train a new SentencePiece tokenizer for the threshold provided\n",
    "    try:\n",
    "      spm.SentencePieceTrainer.train(input='botchan.txt', model_prefix=f'm{threshold}', vocab_size=threshold)\n",
    "    except RuntimeError:\n",
    "      # Vocabulary size > 5239 raises a runtime error\n",
    "      return 1.0 \n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    # Load our recently trained model\n",
    "    sp.load(f'm{threshold}.model')\n",
    "\n",
    "    # Count the number of times UNK (id=0) was assigned in the entire dataset from the model\n",
    "    total = 0\n",
    "    unk = 0\n",
    "    for line in self.lines:\n",
    "      ids = sp.encode_as_ids(line)\n",
    "      unk += ids.count(0)\n",
    "      total += len(ids)\n",
    "    return (total - unk) / total\n",
    "\n",
    "sp_tokenizer = SentencePieceTokenizer(lines)\n",
    "assert sp_tokenizer.tokenize(SAMPLE_SENTENCE) == ['▁I', \"'\", 'm', '▁learn', 'ing', '▁N', 'L', 'P', '.', '▁A', 'ren', \"'\", 't', '▁my', '▁pro', 'j', 'e', 'c', 't', 's', '▁awe', 'some', '?'], sp_tokenizer.tokenize(SAMPLE_SENTENCE)\n",
    "sp_tokenizer.coverage_report(THRESHOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see the OOV% is really low but at the same time it increased a bit when we increased a vocabulary size. An intuitive way to think about this is that in smaller vocabularies the algorithm trains to something closer to character embeddings as we give it a bit larger size it tries to learn more language semantics and trades off some vocabulary coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉 WOOHOOO we've covered the part 1 of this week. Let's keep making progress and proceed to the Generation notebook. But do come back to try out some of the extensions.\n",
    "\n",
    "# Extensions\n",
    "\n",
    "Now that you've worked through the part 1 of the project there is a lot more for us to try:\n",
    "\n",
    "- Try the new tokenizers in the Week 1 EmbeddingBag model?\n",
    "- Similarly change the tokenizer in the Week 2 LSTM?\n",
    "- Compare the tokenizers on a non-english language data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
