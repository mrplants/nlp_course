{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Agumented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pdf_links(url):\n",
    "    \"\"\"\n",
    "    Finds and returns all the PDF links present in a webpage.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the webpage to scan for PDF links.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of URLs (str) that are linked to PDF files.\n",
    "    \"\"\"\n",
    "    # Send a GET request to the specified URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to load page: {url}\")\n",
    "\n",
    "    # Parse the content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all anchor tags, then filter out those with href ending in '.pdf'\n",
    "    pdf_links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.pdf')]\n",
    "\n",
    "    return pdf_links\n",
    "\n",
    "def download_to_file(pdf_url, filepath):\n",
    "    \"\"\"\n",
    "    Downloads a PDF from the given URL, saving it to the indicated filepath.\n",
    "\n",
    "    Args:\n",
    "    pdf_url (str): The URL from where to download the PDF.\n",
    "    filepath (str): The filepath to save the PDF to.\n",
    "\n",
    "    Returns:\n",
    "    int: Updated total size of the downloaded PDF.\n",
    "    \"\"\"\n",
    "    with requests.get(pdf_url, stream=True) as pdf_response:\n",
    "        if pdf_response.status_code != 200:\n",
    "            print(f\"Failed to download PDF: {pdf_url}\")\n",
    "            return 0\n",
    "\n",
    "        # Create a file to store the PDF\n",
    "        with open(Path(filepath) / os.path.basename(pdf_url), 'wb') as f:\n",
    "            for chunk in pdf_response.iter_content(chunk_size=8192):\n",
    "                if chunk:  # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "\n",
    "            # Update total size\n",
    "            total_size = f.tell()\n",
    "\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0951382e-02  5.7414606e-02 -1.1036388e-02 ...  3.5131579e-05\n",
      "  -2.8092245e-02 -2.1599913e-02]\n",
      " [-1.3367120e-02  2.7091343e-02 -2.3367403e-02 ...  2.8799422e-02\n",
      "  -1.0674847e-02  2.8820729e-02]]\n"
     ]
    }
   ],
   "source": [
    "# language_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "sentences = ['search_query: What is TSNE?', 'search_query: Who is Laurens van der Maaten?']\n",
    "embeddings = embed_model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:26<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of downloaded PDFs: 25.98 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the PDF links from the Moltensalt website\n",
    "url = 'http://moltensalt.org/references/static/downloads/pdf/index.html'\n",
    "num_pdfs_to_download = 10\n",
    "pdf_links = find_pdf_links(url)[:num_pdfs_to_download]\n",
    "total_size = 0\n",
    "download_dir = './pdf_dataset'\n",
    "if not os.path.exists(download_dir):\n",
    "    os.makedirs(download_dir)\n",
    "for pdf_link in tqdm(pdf_links):\n",
    "    total_size += download_to_file(pdf_link, download_dir)\n",
    "\n",
    "print(f'Total size of downloaded PDFs: {total_size / 1e6:.2f} MB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
