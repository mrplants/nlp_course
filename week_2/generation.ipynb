{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Text Generation\n",
    "\n",
    "### What we are building\n",
    "A smart compose system that assists in writing movie reviews using the IMDB movie review dataset. FYI: You probably interact with smart compose multiple times a day while typing in Gmail, typing on your phone, or just using Google search.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "We will compare a really simple memorization model that just remembers how often certain words follow a phrase with a pre-trained GPT-2. Finetuning a GPT-2 can take a long time even with a GPU so we'll leave that as an extension project.\n",
    "\n",
    "### Code Overview\n",
    "\n",
    "- Dependencies: Install and import python dependencies\n",
    "- Datasets - Methods and dataset for evaluation\n",
    "- Models\n",
    "  - Memorization\n",
    "  - GPT-2 Pretrained\n",
    "- Extensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "âœ¨ Now let's get started! To kick things off, as always, we will install some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the relevant libraries\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset_builder\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading (common to all solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0379a499650544bdbb3c28dfae83987a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e8c4db5b3b4495aafd709cb439a288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b40f075f184101aa07c723218128e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8f3584659741699454a5649735a6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6b118ce6fd4f22ac49b99c7394cb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e242bfdf3237435ab551d650b1e2ad72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9763f2c929b4f32873d418657b0f174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data: 25000\n",
      "Length of test data: 25000\n"
     ]
    }
   ],
   "source": [
    "dataset_builder = load_dataset_builder('imdb')\n",
    "train_dataset = [d[\"text\"] for d in load_dataset('imdb', split='train')]\n",
    "test_dataset = [d[\"text\"] for d in load_dataset('imdb', split='test')]\n",
    "\n",
    "print(f\"Length of training data: {len(train_dataset)}\")\n",
    "print(f\"Length of test data: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Dataset\n",
    "\n",
    "Running GPT-2 is really expensive so we create a small sample dataset of size 500 and use that for our evaluations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "def create_eval_dataset(dataset, num_examples=500):\n",
    "  if len(dataset) < num_examples:\n",
    "    raise ValueError(f\"Can not select {num_examples} unique examples from dataset of size {len(dataset)}\")\n",
    "\n",
    "  # Since it is really expensive to run GPT, we'll use a smaller dataset for eval\n",
    "  sample = np.random.choice(dataset, num_examples, replace=False)\n",
    "\n",
    "  prefixes = []\n",
    "  output_words = []\n",
    "  for d in sample:\n",
    "    words = d.lower().split(\" \")\n",
    "    boundary = np.random.randint(1, len(words)-1)\n",
    "    prefix = \" \".join(words[:boundary])\n",
    "    prefixes.append(prefix)\n",
    "    output_words.append(words[boundary])\n",
    "  return prefixes, output_words\n",
    "\n",
    "prefixes, output_words = create_eval_dataset(test_dataset, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Function**: Create a single function to compute correct predictions in the top_k from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_exact_match_at(model, prefixes, output_words, top_k):\n",
    "  em_count = 0\n",
    "  i = 0\n",
    "  for i, (prefix, output_word) in enumerate(zip(prefixes, output_words)):\n",
    "    for p in model.predict(prefix, top_k):\n",
    "      if p.strip() == output_word.strip():\n",
    "        em_count += 1\n",
    "        break\n",
    "    if i % 20 == 0:\n",
    "      print(f\"Evaluated {i} prefixes\")\n",
    "  print(f\"Exact match evaluation em@{top_k}:{em_count /len(prefixes)} . Model got {em_count} matches out of {len(prefixes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memorizer\n",
    "\n",
    "Model takes the largest prefix it will memorize which defaults to 3. This means for each sentence of the 4 words such as `I like learning NLP` it'll memorize that it saw `NLP` follow the prefix `I like learning` once.  \n",
    "\n",
    "The model also memorizes any window of size between 1 to the largest_prefix length that are fall back options if we encounter new words. So following our example the model has learned the following:\n",
    "\n",
    "```python\n",
    "[\n",
    "  ('I like learning', 'NLP'),\n",
    "  ('I like', 'learning'), ('like learning', 'NLP'),\n",
    "  ('I', 'like'), ('like', 'learning'), ('learning', 'NLP'),\n",
    "]\n",
    "```\n",
    "\n",
    "This is done so that if we encounter a sentence like `We like learning` we can fall back to the prefix of length 2 and then predict `NLP`.\n",
    "\n",
    "**Implement** the predict function that checks from the largest to the smaller possible prefix and uses the memory dictionary to make predictions and returns the top_k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSIGNMENT PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _window(seq, n=2):\n",
    "  \"\"\"Returns a sliding window based on n\n",
    "  \"\"\"\n",
    "  seq = tuple(seq)\n",
    "  if len(seq) < n: \n",
    "    return []\n",
    "  for i in range(0, len(seq) - n + 1):\n",
    "    yield seq[i:i+n]\n",
    "\n",
    "\n",
    "class Memorizer:\n",
    "  def __init__(self, train_dataset, largest_prefix=3): \n",
    "    self.largest_prefix = largest_prefix\n",
    "    self.memory = {}\n",
    "    # Build the dictionaries for each prefix length\n",
    "    for prefix_size in range(largest_prefix+1):\n",
    "      self.memory[prefix_size] = defaultdict(Counter)\n",
    "      self._build(train_dataset, prefix_size + 1, self.memory[prefix_size])\n",
    "\n",
    "  def _build(self, train_dataset, window_size, memory):\n",
    "    \"\"\"Build the memory dictionary for a provided window_size\n",
    "    \"\"\"\n",
    "    for data in train_dataset:\n",
    "      words = data.split(\" \")\n",
    "      # Compute the different word windows using the _window function\n",
    "      for window in _window(words, window_size):\n",
    "        if window_size == 1:\n",
    "          # There is no window, just memorize how frequently each word occurs in the dataset\n",
    "          output_word = window[0]\n",
    "          # Default all the prefixes to UNK\n",
    "          prefix = \"UNK\"\n",
    "        else:\n",
    "          # Use the prefix and update the count of the word that follows it\n",
    "          prefix = \" \".join(window[:-1])\n",
    "          output_word = window[-1]\n",
    "        memory[prefix][output_word] += 1\n",
    "\n",
    "  def predict(self, prefix, top_k=1):\n",
    "    \"\"\"Top_k words that might follow the given the prefix in our dataset\n",
    "    \"\"\"\n",
    "    prefix_words = prefix.split(\" \")\n",
    "    for prefix_len in range(min(len(prefix_words), self.largest_prefix), 0, -1):\n",
    "\n",
    "      # Compute the prefix string for the size of the window\n",
    "      ### TO BE COMPLETED ### \n",
    "      prefix_str = \" \".join(prefix_words[-prefix_len:])\n",
    "      ### TO BE COMPLETED ### \n",
    "\n",
    "      # If prefix is in memory \"return\" the top_k matches \n",
    "      # Remember we've to return here since we want to use the data from the longest prefix that matches\n",
    "      if prefix_str in self.memory[prefix_len]:\n",
    "        ### TO BE COMPLETED ### \n",
    "        return [p[0] for p in self.memory[prefix_len][prefix_str].most_common(top_k)]\n",
    "        ### TO BE COMPLETED ###\n",
    "\n",
    "    # None of the prefix matched so just return the most common words in the dataset\n",
    "    predictions = self.memory[0][\"UNK\"].most_common(top_k)\n",
    "    return [p[0] for p in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Memorizer widget\n",
    "\n",
    "Ha! Here is a cute trick to build fun widgets within the colab. Just try different sentences for the dataset and prefix to see if the memorizer is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  ['football', 'tennis']\n"
     ]
    }
   ],
   "source": [
    "#@title Experiment with Memorizer\n",
    "\"\"\"\n",
    "In this cell, we've built a toy dataset with only 3 examples. \n",
    "Now given the prefix 'I like', a trie would emit 'football' and 'tennis' based \n",
    "on the co-occurence.\n",
    "\"\"\"\n",
    "dataset_1 = \"I like football\" #@param {type:\"string\"}\n",
    "dataset_2 = \"I like tennis sometimes\" #@param {type:\"string\"}\n",
    "dataset_3 = \"I like football way too much\" #@param {type:\"string\"}\n",
    "prefix = \"I like\" #@param {type:\"string\"}\n",
    "\n",
    "memorized_toy_model = Memorizer([dataset_1, dataset_2, dataset_3])\n",
    "\n",
    "predictions = memorized_toy_model.predict(prefix, 2)\n",
    "\n",
    "# The model should predict [football, tennis]\n",
    "# since football occurred twice while tennis was just once.\n",
    "print(\"Predictions: \", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train memorizer on the actual training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "memorized_model = Memorizer(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on top_1 and top_3\n",
    "##### <font color='red'>Expected em@1: ~0.146%</font>\n",
    "##### <font color='red'>Expected em@3: ~0.222%</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 0 prefixes\n",
      "Evaluated 20 prefixes\n",
      "Evaluated 40 prefixes\n",
      "Evaluated 60 prefixes\n",
      "Evaluated 80 prefixes\n",
      "Evaluated 100 prefixes\n",
      "Evaluated 120 prefixes\n",
      "Evaluated 140 prefixes\n",
      "Evaluated 160 prefixes\n",
      "Evaluated 180 prefixes\n",
      "Evaluated 200 prefixes\n",
      "Evaluated 220 prefixes\n",
      "Evaluated 240 prefixes\n",
      "Evaluated 260 prefixes\n",
      "Evaluated 280 prefixes\n",
      "Evaluated 300 prefixes\n",
      "Evaluated 320 prefixes\n",
      "Evaluated 340 prefixes\n",
      "Evaluated 360 prefixes\n",
      "Evaluated 380 prefixes\n",
      "Evaluated 400 prefixes\n",
      "Evaluated 420 prefixes\n",
      "Evaluated 440 prefixes\n",
      "Evaluated 460 prefixes\n",
      "Evaluated 480 prefixes\n",
      "Exact match evaluation em@1:0.146 . Model got 73 matches out of 500\n"
     ]
    }
   ],
   "source": [
    "evaluate_exact_match_at(memorized_model, prefixes, output_words, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 0 prefixes\n",
      "Evaluated 20 prefixes\n",
      "Evaluated 40 prefixes\n",
      "Evaluated 60 prefixes\n",
      "Evaluated 80 prefixes\n",
      "Evaluated 100 prefixes\n",
      "Evaluated 120 prefixes\n",
      "Evaluated 140 prefixes\n",
      "Evaluated 160 prefixes\n",
      "Evaluated 180 prefixes\n",
      "Evaluated 200 prefixes\n",
      "Evaluated 220 prefixes\n",
      "Evaluated 240 prefixes\n",
      "Evaluated 260 prefixes\n",
      "Evaluated 280 prefixes\n",
      "Evaluated 300 prefixes\n",
      "Evaluated 320 prefixes\n",
      "Evaluated 340 prefixes\n",
      "Evaluated 360 prefixes\n",
      "Evaluated 380 prefixes\n",
      "Evaluated 400 prefixes\n",
      "Evaluated 420 prefixes\n",
      "Evaluated 440 prefixes\n",
      "Evaluated 460 prefixes\n",
      "Evaluated 480 prefixes\n",
      "Exact match evaluation em@3:0.222 . Model got 111 matches out of 500\n"
     ]
    }
   ],
   "source": [
    "evaluate_exact_match_at(memorized_model, prefixes, output_words, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2: Generative Pre-trained Transformer\n",
    "\n",
    "We'll use the pretrainined GPT-2 model provided by the transformers package. Make sure you implement the predict function.\n",
    "\n",
    "Implementation Steps:\n",
    "1. Encode the sentence using `tokenizer.encode` and make sure it returns a torch tensor.\n",
    "2. Run this through the model and those are your predictions.\n",
    "3. Decode the indices from the output of top_k using the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSIGNMENT PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2PreTrained:\n",
    "  def __init__(self): \n",
    "    self.tokenizer = GPT2TokenizerFast.from_pretrained('distilgpt2')\n",
    "    self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    self.model.eval()\n",
    "\n",
    "  def predict(self, prefix, top_k=1):\n",
    "    ### TO BE IMPLEMENTED ### \n",
    "    indexed_tokens = self.tokenizer.encode(prefix)\n",
    "    # restrict the length of the sequence to 1024 tokens\n",
    "    indexed_tokens = indexed_tokens[:1023]\n",
    "    predictions = self.model(torch.tensor([indexed_tokens]))\n",
    "    ### TO BE IMPLEMENTED ### \n",
    "\n",
    "    _, indices = torch.topk(predictions[0][0, -1, :], k=top_k)\n",
    "    ### TO BE IMPLEMENTED ### \n",
    "    predictions = [self.tokenizer.decode([ind.item()]) for ind in indices]\n",
    "    ### TO BE IMPLEMENTED ### \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with GPT-2 Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [' pitch', ' ball']\n"
     ]
    }
   ],
   "source": [
    "#@title Experiment with GPT-2\n",
    "\"\"\"\n",
    "In this cell, we've built a toy prompt from which we predict \n",
    "the next words using GPT-2.\n",
    "\"\"\"\n",
    "text = \"pitcher threw a\" #@param {type:\"string\"}\n",
    "\n",
    "gpt_model = GPT2PreTrained()\n",
    "\n",
    "predictions = gpt_model.predict(text, 2)\n",
    "## Output should be \"pitch, ball\" or something similar\n",
    "print(\"Predictions: \", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on top_1 and top_3\n",
    "##### <font color='red'>Expected em@1: ~0.226%</font>\n",
    "##### <font color='red'>Expected em@3: ~0.338%</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 0 prefixes\n",
      "Evaluated 20 prefixes\n",
      "Evaluated 40 prefixes\n",
      "Evaluated 60 prefixes\n",
      "Evaluated 80 prefixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1120 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 100 prefixes\n",
      "Evaluated 120 prefixes\n",
      "Evaluated 140 prefixes\n",
      "Evaluated 160 prefixes\n",
      "Evaluated 180 prefixes\n",
      "Evaluated 200 prefixes\n",
      "Evaluated 220 prefixes\n",
      "Evaluated 240 prefixes\n",
      "Evaluated 260 prefixes\n",
      "Evaluated 280 prefixes\n",
      "Evaluated 300 prefixes\n",
      "Evaluated 320 prefixes\n",
      "Evaluated 340 prefixes\n",
      "Evaluated 360 prefixes\n",
      "Evaluated 380 prefixes\n",
      "Evaluated 400 prefixes\n",
      "Evaluated 420 prefixes\n",
      "Evaluated 440 prefixes\n",
      "Evaluated 460 prefixes\n",
      "Evaluated 480 prefixes\n",
      "Exact match evaluation em@1:0.226 . Model got 113 matches out of 500\n"
     ]
    }
   ],
   "source": [
    "evaluate_exact_match_at(gpt_model, prefixes, output_words, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 0 prefixes\n",
      "Evaluated 20 prefixes\n",
      "Evaluated 40 prefixes\n",
      "Evaluated 60 prefixes\n",
      "Evaluated 80 prefixes\n",
      "Evaluated 100 prefixes\n",
      "Evaluated 120 prefixes\n",
      "Evaluated 140 prefixes\n",
      "Evaluated 160 prefixes\n",
      "Evaluated 180 prefixes\n",
      "Evaluated 200 prefixes\n",
      "Evaluated 220 prefixes\n",
      "Evaluated 240 prefixes\n",
      "Evaluated 260 prefixes\n",
      "Evaluated 280 prefixes\n",
      "Evaluated 300 prefixes\n",
      "Evaluated 320 prefixes\n",
      "Evaluated 340 prefixes\n",
      "Evaluated 360 prefixes\n",
      "Evaluated 380 prefixes\n",
      "Evaluated 400 prefixes\n",
      "Evaluated 420 prefixes\n",
      "Evaluated 440 prefixes\n",
      "Evaluated 460 prefixes\n",
      "Evaluated 480 prefixes\n",
      "Exact match evaluation em@3:0.338 . Model got 169 matches out of 500\n"
     ]
    }
   ],
   "source": [
    "evaluate_exact_match_at(gpt_model, prefixes, output_words, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ‰ YAYYYY!!! We did it, that's it. Take a second to pause how many different things you've tried in the last 4 weeks. Go you!!\n",
    "\n",
    "## Extensions\n",
    "- Build an LSTM based generation model (Remember to cut sequences at about 10-15 words, LSTMs don't work on long sentences).\n",
    "- Try fine-tuning the GPT-2 model using a GPU runtime for the notebook. (NOTE: colab free GPUs are pretty bad so this is probably not worth doing in the free tier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
