{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
    "\n",
    "# Prereq Week: Text Classification\n",
    "\n",
    "### What are we building\n",
    "We’ll continue to apply our learning philosophy of repetition as we build multiple classification models of increasing complexity in the following order:\n",
    "\n",
    "1. Average of Word2Vec + MLP Layer\n",
    "1. Can we concatenate 3 token embeddings and then average them? Does this do better than the previous method?\n",
    "1. Build an embedding layer based model.\n",
    "1. **Extension**: Explore different parameters, features and architectures. \n",
    "\n",
    "###  Evaluation\n",
    "We’ll be evaluating our models on the following metric: \n",
    "\n",
    "1. Accuracy: is the ratio of the number of correctly classified instances to the total number of instances\n",
    "1. **Extension**: this is a multi-class classification problem, visualize a [confusion matrix](https://torchmetrics.readthedocs.io/en/latest/references/functional.html#confusion-matrix-func) of N*N of actual class vs predicted class (N = number of classes).\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. We've provide scaffolding for all the boiler plate PyTorch code to get to our first model. This covers downloading and parsing the dataset, training code for the baseline model. **Make sure to read all the steps and internalize what is happening**.\n",
    "1. At this point our model gets to an accuracy of about 0.32. After this we'll try to improve the model by using sliding windows of text instead of just one word at a time. **Does this improve accuracy?**\n",
    "1. The third model we're going to build is an embedding layer based model. Here instead of using pre-trained word-embeddings we'll be creating new vectors as part of the training process. **How do you think this model will perform?**\n",
    "1. **Extension**: We've suggested a bunch of extensions to the project so go crazy, tweak any parts of the pipeline and see if you can beat all the current modes.\n",
    "\n",
    "### Code Overview\n",
    "- Dependencies: Python dependencies and loading the spacy model\n",
    "- Project\n",
    "  - Dataset: Download the conversation dataset and parse it into a pytorch Dataset\n",
    "  - Trainer: Trainer function to help with multi-epoch training\n",
    "  - Model 1: Simple Word2Vec + MLP model\n",
    "  - Model 2: Sliding window trigram (Word2Vec)\n",
    "  - Model 3: Embedding bag based model on Trigram\n",
    "- Extensions\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tfdata\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "# python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Fix the random seed so that we get consistent results\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Project\n",
    "✨ Let's Begin ✨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Processing (Common to ALL Solutions)\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "We’ll be using the Empathetic Dialogs dataset open-sourced by Facebook ([link](https://research.fb.com/publications/towards-empathetic-open-domain-conversation-models-a-new-benchmark-and-dataset/)). It can be downloaded as a tar ball from the following [link](https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz)\n",
    "\n",
    "A sample row from the dataset: \n",
    "```\n",
    "conv_id,utterance_idx,context,prompt,speaker_idx,utterance,selfeval,tags\n",
    "hit:12388_conv:24777,1,joyful,I felt overcome with emotions when Christmas came around as a kid,437,Christmas was the best time of year back in the day!,5|5|5_5|5|5, ''\n",
    "```\n",
    "\n",
    "Let's download and explore the dataset and these should automatically get clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dataset and Data loaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html): Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "[LightingDataModule](https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html#datamodules): A datamodule is a shareable, reusable class that encapsulates all the steps needed to process data. A datamodule encapsulates the five steps involved in data processing in PyTorch:\n",
    "\n",
    "1. Download / tokenize / process.\n",
    "2. Clean and (maybe) save to disk.\n",
    "3. Load inside Dataset.\n",
    "4. Apply transforms (rotate, tokenize, etc…).\n",
    "5. Wrap inside a DataLoader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# DATASET #\n",
    "###########\n",
    "\n",
    "class EmpatheticDataset(tfdata.Dataset):\n",
    "    def __init__(self, data_dir: str = 'classification_data', split: str = 'train', transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset_url = 'https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz'\n",
    "        self.directory_name = 'empatheticdialogues'\n",
    "        self.transform = transform\n",
    "        # Check if the dataset directory already exists to avoid re-downloading\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir, exist_ok=True)\n",
    "            # Download the dataset using wget\n",
    "            subprocess.run(['wget', '-q', self.dataset_url, '-O', 'empatheticdialogues.tar.gz'])\n",
    "            # Extract the dataset\n",
    "            subprocess.run(['tar', '-xvf', 'empatheticdialogues.tar.gz', '-C', self.data_dir])\n",
    "            # Remove the tar file to clean up\n",
    "            os.remove('empatheticdialogues.tar.gz')\n",
    "        else:\n",
    "            print(\"Dataset already downloaded and extracted.\")\n",
    "        train_data_url = f\"{self.data_dir}/{self.directory_name}/train.csv\"\n",
    "        val_data_url = f\"{self.data_dir}/{self.directory_name}/valid.csv\"\n",
    "        test_data_url = f\"{self.data_dir}/{self.directory_name}/test.csv\"\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(pd.read_csv(train_data_url, on_bad_lines='skip')['context'])\n",
    "\n",
    "        if split == 'train':\n",
    "            self.pd_data = pd.read_csv(train_data_url, on_bad_lines='skip')\n",
    "        elif split == 'val':\n",
    "            self.pd_data = pd.read_csv(val_data_url, on_bad_lines='skip')\n",
    "        elif split == 'test':\n",
    "            self.pd_data = pd.read_csv(test_data_url, on_bad_lines='skip')\n",
    "        else:\n",
    "            raise ValueError(\"Invalid split. Must be one of 'train', 'val', or 'test'.\")\n",
    "        \n",
    "        # Add a new column for the index of the context\n",
    "        self.pd_data['context_idx'] = self.label_encoder.transform(self.pd_data['context'])\n",
    "\n",
    "        # Apply the transform function to the data\n",
    "        self.data = [self.transform(item) if self.transform else item for _, item in tqdm(list(self.pd_data.iterrows()), \"Applying dataset transform\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's poke around our dataset a little!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:00<00:00, 6079611.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_id</th>\n",
       "      <th>utterance_idx</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>speaker_idx</th>\n",
       "      <th>utterance</th>\n",
       "      <th>selfeval</th>\n",
       "      <th>tags</th>\n",
       "      <th>context_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>1</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>I remember going to see the fireworks with my ...</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>2</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>0</td>\n",
       "      <td>Was this a friend you were in love with_comma_...</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>3</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>This was a best friend. I miss her.</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>4</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>0</td>\n",
       "      <td>Where has she gone?</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>5</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>We no longer talk.</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76663</th>\n",
       "      <td>hit:12424_conv:24848</td>\n",
       "      <td>5</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I found some pictures of my grandma in the att...</td>\n",
       "      <td>389</td>\n",
       "      <td>Yeah reminds me of the good old days.  I miss ...</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76664</th>\n",
       "      <td>hit:12424_conv:24849</td>\n",
       "      <td>1</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>294</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76665</th>\n",
       "      <td>hit:12424_conv:24849</td>\n",
       "      <td>2</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>389</td>\n",
       "      <td>Oh hey that's awesome!  That is awesome right?</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76666</th>\n",
       "      <td>hit:12424_conv:24849</td>\n",
       "      <td>3</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>294</td>\n",
       "      <td>It is soooo awesome.  We have been wanting a b...</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76667</th>\n",
       "      <td>hit:12424_conv:24849</td>\n",
       "      <td>4</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>389</td>\n",
       "      <td>That is awesome!!!! Congratulations!</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76668 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    conv_id  utterance_idx      context  \\\n",
       "0              hit:0_conv:1              1  sentimental   \n",
       "1              hit:0_conv:1              2  sentimental   \n",
       "2              hit:0_conv:1              3  sentimental   \n",
       "3              hit:0_conv:1              4  sentimental   \n",
       "4              hit:0_conv:1              5  sentimental   \n",
       "...                     ...            ...          ...   \n",
       "76663  hit:12424_conv:24848              5  sentimental   \n",
       "76664  hit:12424_conv:24849              1    surprised   \n",
       "76665  hit:12424_conv:24849              2    surprised   \n",
       "76666  hit:12424_conv:24849              3    surprised   \n",
       "76667  hit:12424_conv:24849              4    surprised   \n",
       "\n",
       "                                                  prompt  speaker_idx  \\\n",
       "0      I remember going to the fireworks with my best...            1   \n",
       "1      I remember going to the fireworks with my best...            0   \n",
       "2      I remember going to the fireworks with my best...            1   \n",
       "3      I remember going to the fireworks with my best...            0   \n",
       "4      I remember going to the fireworks with my best...            1   \n",
       "...                                                  ...          ...   \n",
       "76663  I found some pictures of my grandma in the att...          389   \n",
       "76664  I woke up this morning to my wife telling me s...          294   \n",
       "76665  I woke up this morning to my wife telling me s...          389   \n",
       "76666  I woke up this morning to my wife telling me s...          294   \n",
       "76667  I woke up this morning to my wife telling me s...          389   \n",
       "\n",
       "                                               utterance     selfeval tags  \\\n",
       "0      I remember going to see the fireworks with my ...  5|5|5_2|2|5  NaN   \n",
       "1      Was this a friend you were in love with_comma_...  5|5|5_2|2|5  NaN   \n",
       "2                    This was a best friend. I miss her.  5|5|5_2|2|5  NaN   \n",
       "3                                    Where has she gone?  5|5|5_2|2|5  NaN   \n",
       "4                                     We no longer talk.  5|5|5_2|2|5  NaN   \n",
       "...                                                  ...          ...  ...   \n",
       "76663  Yeah reminds me of the good old days.  I miss ...  5|5|5_5|5|5  NaN   \n",
       "76664  I woke up this morning to my wife telling me s...  5|5|5_5|5|5  NaN   \n",
       "76665     Oh hey that's awesome!  That is awesome right?  5|5|5_5|5|5  NaN   \n",
       "76666  It is soooo awesome.  We have been wanting a b...  5|5|5_5|5|5  NaN   \n",
       "76667               That is awesome!!!! Congratulations!  5|5|5_5|5|5  NaN   \n",
       "\n",
       "       context_idx  \n",
       "0               28  \n",
       "1               28  \n",
       "2               28  \n",
       "3               28  \n",
       "4               28  \n",
       "...            ...  \n",
       "76663           28  \n",
       "76664           29  \n",
       "76665           29  \n",
       "76666           29  \n",
       "76667           29  \n",
       "\n",
       "[76668 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the internal Pandas DataFrame within the EmpatheticDataset\n",
    "sample_dataset = EmpatheticDataset()\n",
    "sample_dataset.pd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the label encoder in the data module.  It should be able to convert the string labels to integers and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoder classes: ['afraid' 'angry' 'annoyed' 'anticipating' 'anxious' 'apprehensive'\n",
      " 'ashamed' 'caring' 'confident' 'content' 'devastated' 'disappointed'\n",
      " 'disgusted' 'embarrassed' 'excited' 'faithful' 'furious' 'grateful'\n",
      " 'guilty' 'hopeful' 'impressed' 'jealous' 'joyful' 'lonely' 'nostalgic'\n",
      " 'prepared' 'proud' 'sad' 'sentimental' 'surprised' 'terrified' 'trusting']\n",
      "Label for \"sad\": [27]\n",
      "Label for \"hopeful\": [19]\n",
      "Label for \"angry\": [1]\n"
     ]
    }
   ],
   "source": [
    "print(f'Label encoder classes: {sample_dataset.label_encoder.classes_}')\n",
    "print(f'Label for \"sad\": {sample_dataset.label_encoder.transform([\"sad\"])}')\n",
    "print(f'Label for \"hopeful\": {sample_dataset.label_encoder.transform([\"hopeful\"])}')\n",
    "print(f'Label for \"angry\": {sample_dataset.label_encoder.transform([\"angry\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "Now, let's create a transform to extract the context and the utterance from the dataset.\n",
    "\n",
    "The columns we care about are:\n",
    "1. \"context\": This is the emotion we're trying to predict (this has already been converted to a number usign the dataset label encoder)\n",
    "1. \"prompt\" and \"utterance\": We'll combine these sentences and use them as input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_mean_vectors(item):\n",
    "    # Combine 'prompt' and 'utterance' into a single string\n",
    "    input_string = item['prompt'] + ' ' + item['utterance']\n",
    "    # Vectorize the input string\n",
    "    x = np.mean([token.vector for token in nlp.make_doc(input_string)], axis=0)\n",
    "    # Retrieve the context label\n",
    "    y = item['context_idx'] # This column was added in the EmpatheticDataset class __init__ method\n",
    "    return {\n",
    "        'input_string': input_string, # This is useful for visualization/debugging\n",
    "        'sentiment': item['context'], # This is useful for visualization/debugging\n",
    "        'input_vector': x,\n",
    "        'sentiment_idx': y\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate\n",
    "Next, we'll create a collate function that tokenizes the batch and returns the tokenized input and the emotion label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # A torch data loader will combine a list of samples into a batch\n",
    "    # This function will be used to process the batch\n",
    "    # batch is a list of the outputs from the __getitem__ method of the EmpatheticDataset class\n",
    "    # Separate the batch into input and target, then convert to tensors\n",
    "    input_tensors = torch.stack([torch.tensor(item['input_vector']) for item in batch])\n",
    "    target_tensors = torch.stack([torch.tensor(item['sentiment_idx']) for item in batch])\n",
    "    return {\n",
    "        'input': input_tensors,\n",
    "        'target': target_tensors,\n",
    "        'input_string': [item['input_string'] for item in batch], # This is useful for visualization/debugging\n",
    "        'sentiment': [item['sentiment'] for item in batch] # This is useful for visualization/debugging\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmpatheticDialoguesDataModule(L.LightningDataModule):\n",
    "    def __init__(self, batch_size=32, collate_fn=None, transform=None):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.collate_fn = collate_fn\n",
    "        self.transform = transform\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # This downloads the dataset and prepares it\n",
    "        # It doesn't save anything to the data module, just prepares the dataset\n",
    "        EmpatheticDataset()\n",
    "        self.label_encoder = EmpatheticDataset().label_encoder\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # Retrieve the dataset from disk\n",
    "        self.train_dataset = EmpatheticDataset(split='train', transform=self.transform)\n",
    "        self.val_dataset = EmpatheticDataset(split='val', transform=self.transform)\n",
    "        self.test_dataset = EmpatheticDataset(split='test', transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return tfdata.DataLoader(self.train_dataset,\n",
    "                               batch_size=self.batch_size,\n",
    "                               collate_fn=self.collate_fn,\n",
    "                               num_workers=0,\n",
    "                               shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return tfdata.DataLoader(self.val_dataset,\n",
    "                               batch_size=self.batch_size,\n",
    "                               collate_fn=self.collate_fn,\n",
    "                               num_workers=0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return tfdata.DataLoader(self.test_dataset,\n",
    "                               batch_size=self.batch_size,\n",
    "                               collate_fn=self.collate_fn,\n",
    "                               num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classfier Module\n",
    "\n",
    "We've now created the DataLoader and Datasets we'll use in the entire project, it is time to write the training and testing code via a `LightningModule`. \n",
    "\n",
    "[LightingModule](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html): organizes your PyTorch code into 5 sections\n",
    "\n",
    "1. Computations (init).\n",
    "2. Train loop (training_step)\n",
    "3. Validation loop (validation_step)\n",
    "4. Test loop (test_step)\n",
    "5. Optimizers (configure_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier(L.LightningModule):\n",
    "  def __init__(self, model, batch_size, learning_rate, num_classes):\n",
    "      super().__init__()\n",
    "      self.model = model\n",
    "      self.batch_size = batch_size\n",
    "      self.learning_rate = learning_rate\n",
    "      self.accuracy = MulticlassAccuracy(num_classes=num_classes)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x = batch[\"input\"]\n",
    "    y = batch[\"target\"]\n",
    "    y_hat = self.model(x)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "    self.log_dict(\n",
    "      {'train_loss': loss},\n",
    "      batch_size=self.batch_size,\n",
    "      prog_bar=True\n",
    "    )\n",
    "    return loss\n",
    "  \n",
    "  def validation_step(self, batch, batch_nb):\n",
    "    x = batch[\"input\"]\n",
    "    y = batch[\"target\"]\n",
    "    y_hat = self.model(x)\n",
    "    val_loss = F.cross_entropy(y_hat, y)\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    self.log_dict(\n",
    "        {\n",
    "          'val_loss': val_loss,\n",
    "          'val_accuracy': self.accuracy(predictions, y)\n",
    "        },\n",
    "        batch_size=self.batch_size,\n",
    "        prog_bar=True\n",
    "      )\n",
    "    return val_loss\n",
    "\n",
    "  def test_step(self, batch, batch_nb):\n",
    "    x = batch[\"input\"]\n",
    "    y = batch[\"target\"]\n",
    "    y_hat = self.model(x)\n",
    "    test_loss = F.cross_entropy(y_hat, y)\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    self.log_dict(\n",
    "        {\n",
    "          'test_loss': test_loss,\n",
    "          'test_accuracy': self.accuracy(predictions, y)\n",
    "        },\n",
    "        batch_size=self.batch_size,\n",
    "        prog_bar=True\n",
    "      )\n",
    "    return test_loss\n",
    "  \n",
    "  def predict_step(self, batch, batch_idx):\n",
    "    y_hat = self.model(batch[\"input\"])\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    return {'logits':y_hat, 'predictions': predictions, 'sentiment_labels': batch[\"sentiment\"], 'input_string': batch['input_string']}\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Are we building models yet? Finally the time has come to build our baseline model and then we'll work towards improving it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Average word vector of the sentence -- Baseline\n",
    "##### <font color='red'>Expected accuracy: ~29 - 32%</font>\n",
    "\n",
    "Let's build our first simple word2vec based model we'll use as our baseline.\n",
    "\n",
    "Here we've three key pieces:\n",
    "\n",
    "1. *WordVectorClassificationModel*: Simple linear model that just has one single neuron layer that maps the input word2vec dimensions (300) to the output classes (32) building a really simple classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVectorClassificationModel(torch.nn.Module):\n",
    "  def __init__(self, word_vec_dimension, num_classes):\n",
    "    super().__init__()\n",
    "    self.linear_layer = torch.nn.Linear(word_vec_dimension, num_classes)\n",
    "\n",
    "  # 🌟🌟🌟 Pay extra attention here since you'll have to work on this in the models 🌟🌟🌟\n",
    "  def forward(self, batch):\n",
    "    \"\"\"Projection from word_vec_dim to n_classes\n",
    "\n",
    "    Batch is of shape (batch_size, max_seq_len, word_vector_dim)\n",
    "    \"\"\"\n",
    "    return self.linear_layer(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "Now, let's use the Lighning [`Trainer`](https://lightning.ai/docs/pytorch/latest/common/trainer.html) to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    transform = transform_mean_vectors,\n",
    "    batch_size=32,\n",
    "    max_epochs=4,\n",
    "    learning_rate=0.001,\n",
    "):\n",
    "    # Create a pytorch trainer\n",
    "    trainer = L.Trainer(max_epochs=max_epochs, check_val_every_n_epoch=1)\n",
    "\n",
    "    # Initialize our data loader with the passed vectorizer\n",
    "    data_module = EmpatheticDialoguesDataModule(collate_fn=collate_fn,\n",
    "                                                transform=transform,\n",
    "                                                batch_size=batch_size)\n",
    "    data_module.prepare_data()\n",
    "    data_module.setup('fit')\n",
    "\n",
    "    # Instantiate a new lightning module\n",
    "    module = EmotionClassifier(model,\n",
    "                            batch_size=batch_size,\n",
    "                            learning_rate=learning_rate,\n",
    "                            num_classes=len(data_module.label_encoder.classes_))\n",
    "\n",
    "    # Train and validate the model\n",
    "    trainer.fit(module, data_module.train_dataloader(), val_dataloaders=data_module.val_dataloader())\n",
    "\n",
    "    # Test the model\n",
    "    trainer.test(module, data_module.test_dataloader())\n",
    "\n",
    "    # Predict on the same test set to show some output\n",
    "    output = trainer.predict(module, data_module.test_dataloader())\n",
    "\n",
    "    # Un-collate the output\n",
    "    uncollated_output = {}\n",
    "    for batch_output in output:\n",
    "        for k, v in batch_output.items():\n",
    "            if k not in uncollated_output:\n",
    "                uncollated_output[k] = []\n",
    "            uncollated_output[k].extend(v)\n",
    "\n",
    "    return uncollated_output, data_module.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:00<00:00, 6015018.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:00<00:00, 6057280.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:06<00:00, 11585.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 6318/6318 [00:00<00:00, 10250.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 5701/5701 [00:00<00:00, 9589.70it/s] \n",
      "\n",
      "  | Name     | Type                          | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model    | WordVectorClassificationModel | 9.6 K \n",
      "1 | accuracy | MulticlassAccuracy            | 0     \n",
      "-----------------------------------------------------------\n",
      "9.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.6 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 300/300 [00:02<00:00, 119.39it/s, v_num=30, train_loss=2.250, val_loss=2.330, val_accuracy=0.334]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 300/300 [00:02<00:00, 119.25it/s, v_num=30, train_loss=2.250, val_loss=2.330, val_accuracy=0.334]\n",
      "Testing DataLoader 0: 100%|██████████| 23/23 [00:00<00:00, 84.94it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.3234937787055969\n",
      "        test_loss            2.317218542098999\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Predicting DataLoader 0: 100%|██████████| 23/23 [00:00<00:00, 251.05it/s]\n",
      "Input: I told my brother a secret and I hope he keeps it! I told my brother that I was planning to propose to my best friend_comma_ but I hope he keeps it a secret!\n",
      "Predicted label: ['trusting']\n",
      "Actual label: trusting\n",
      "\n",
      "Input: Playing with puppies. It was definitely exciting for me! I haven't been able to play with a dog since my own passed away.\n",
      "Predicted label: ['excited']\n",
      "Actual label: excited\n",
      "\n",
      "Input: I have an amazing husband who is always there for me no matter what.  He makes me feel so loved and I can't imagine life without him. I have an amazing husband who is always there for me no matter what\n",
      "Predicted label: ['grateful']\n",
      "Actual label: content\n",
      "\n",
      "Input: I had just spend 100 dollars on rolling a slot machine. After I just left and didn't win anything_comma_ someone came up and hit triple 7s at the machine i was just at. I was just at the casino_comma_ spent almost 100 dollars on a single machine. I didn't will anything.\n",
      "Predicted label: ['disappointed']\n",
      "Actual label: jealous\n",
      "\n",
      "Input: yes i am going to coffee shop the man speech is  so guilty thank you\n",
      "Predicted label: ['grateful']\n",
      "Actual label: guilty\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uncollated_output, label_encoder = train(\n",
    "    model=WordVectorClassificationModel(word_vec_dimension=300, num_classes=32),\n",
    "    batch_size=256,\n",
    "    max_epochs=4,\n",
    "    learning_rate=0.001,\n",
    ")\n",
    "\n",
    "# Feel free to explore the uncollated_output dictionary to see the predictions\n",
    "for _ in range(5):\n",
    "    # Randomly select a sample from the test set\n",
    "    i = np.random.randint(len(uncollated_output['input_string']))\n",
    "    print(f'Input: {uncollated_output[\"input_string\"][i]}')\n",
    "    print(f'Predicted label: {label_encoder.inverse_transform([uncollated_output[\"predictions\"][i].item()])}')\n",
    "    print(f'Actual label: {uncollated_output[\"sentiment_labels\"][i]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉🎉🎉 WE HAVE OUR TEXT CLASSIFIER 🎉🎉🎉\n",
    "\n",
    "Now might be a good time to play around with the vectorizer and the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Part: 1 - Model 2: Sliding Window of Vectors ---- TO BE COMPLETED\n",
    "##### <font color='red'>Expected accuracy: ~30 to 36%</font>\n",
    "\n",
    "We'll be re-using the simple linear model from Model-1 but changing the input to use sliding windows instead of one word at a time.\n",
    "\n",
    "Implement a new `transform_sliding_window` which is a variant of the `transform_mean_vectors` that operates. Here are some instructions on how to implement it.\n",
    "\n",
    "1. Split the sentence into chunks of the size of the n_grams parameter.\n",
    "2. Concat all the spacy embeddings of the tokens inside to create embeddings of the chuck. Each chunk vector is of the size of `n_gram * size_of_embedding`\n",
    "3. Sentence vector is the average of all chunk vectors.\n",
    "4. Return the sentence_vector and tokens (for debugging, option: could just return None for tokens)\n",
    "\n",
    "Does this model perform better than our baseline? Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Trainer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 48\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m### TO BE IMPLEMENTED ###\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentence_vector, sentence_tokens\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWordVectorClassificationModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Observe the change in input parameters\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mHParamsSpacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_vec_dimension\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mHParamsSpacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_grams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mHParamsSpacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHParamsSpacy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSpacyChunkVectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHParamsSpacy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Trainer' object is not callable"
     ]
    }
   ],
   "source": [
    "batch_size: int = 32\n",
    "learning_rate: float = 0.001\n",
    "max_epochs: int = 4\n",
    "n_grams: int = 3  ## ADAPT, Change it to your liking\n",
    "\n",
    "def transform_sliding_window(item):\n",
    "    # Combine 'prompt' and 'utterance' into a single string\n",
    "    input_string = item['prompt'] + ' ' + item['utterance']\n",
    "    \"\"\"Given a sentence, tokenize it and calculate a vector for that sentence.\n",
    "\n",
    "    Sentence is of length (n)\n",
    "\n",
    "    1. Split the sentence into tokens using Spacy's function make_doc\n",
    "    2. Split the list of token into size of the n_grams parameter.\n",
    "    3. Concat all the spacy embeddings of the tokens inside to create embeddings of the chuck. Each chunk vector is of the size of n_grams * size_of_embedding\n",
    "    4. Sentence vector is the average of all chunk vectors.\n",
    "    5. Return the sentence_vector and tokens (option: could just return None for tokens)\n",
    "\n",
    "    Sentence_vector is of length (n_grams * word_vector_dim)\n",
    "    Sentence_tokens is of length (n)\n",
    "\n",
    "    Example of word tri-gram encoding: \"I am doing great right now.\":\n",
    "      <EMPTY (300,)> <EMPTY (300,)> <I (300,)> -> (900, )\n",
    "      <EMPTY (300,)> <I (300,)> <am (300,)> -> (900, )\n",
    "      <I (300,)> <am (300,)> <doing (300,)> -> (900, )\n",
    "      <am (300,)> <doing (300,)> <great (300,)> -> (900, )\n",
    "      <doing (300,)> <great (300,)> <right (300,)> -> (900, )\n",
    "      <great (300,)> <right (300,)> <now (300,)> -> (900, )\n",
    "      <right (300,)> <now (300,)> <EMPTY (300,)> -> (900, )\n",
    "      <now (300,)> <EMPTY (300,)> <EMPTY (300,)> -> (900, )\n",
    "\n",
    "    We'd encourage you to also try other variants to encode!\n",
    "    \"\"\"\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    split_sentence = []\n",
    "\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    # Retrieve the context label\n",
    "    y = item['context_idx'] # This column was added in the EmpatheticDataset class __init__ method\n",
    "    return {\n",
    "        'input_string': input_string, # This is useful for visualization/debugging\n",
    "        'sentiment': item['context'], # This is useful for visualization/debugging\n",
    "        'input_vector': x,\n",
    "        'sentiment_idx': y\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
