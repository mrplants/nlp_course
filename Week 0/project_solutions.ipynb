{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
    "\n",
    "# Prereq Week: Text Classification\n",
    "\n",
    "### What are we building\n",
    "We’ll continue to apply our learning philosophy of repetition as we build multiple classification models of increasing complexity in the following order:\n",
    "\n",
    "1. Average of Word2Vec + MLP Layer\n",
    "1. Can we concatenate 3 token embeddings and then average them? Does this do better than the previous method?\n",
    "1. Build an embedding layer based model.\n",
    "1. **Extension**: Explore different parameters, features and architectures. \n",
    "\n",
    "###  Evaluation\n",
    "We’ll be evaluating our models on the following metric: \n",
    "\n",
    "1. Accuracy: is the ratio of the number of correctly classified instances to the total number of instances\n",
    "1. **Extension**: this is a multi-class classification problem, visualize a [confusion matrix](https://torchmetrics.readthedocs.io/en/latest/references/functional.html#confusion-matrix-func) of N*N of actual class vs predicted class (N = number of classes).\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. We've provide scaffolding for all the boiler plate PyTorch code to get to our first model. This covers downloading and parsing the dataset, training code for the baseline model. **Make sure to read all the steps and internalize what is happening**.\n",
    "1. At this point our model gets to an accuracy of about 0.32. After this we'll try to improve the model by using sliding windows of text instead of just one word at a time. **Does this improve accuracy?**\n",
    "1. The third model we're going to build is an embedding layer based model. Here instead of using pre-trained word-embeddings we'll be creating new vectors as part of the training process. **How do you think this model will perform?**\n",
    "1. **Extension**: We've suggested a bunch of extensions to the project so go crazy, tweak any parts of the pipeline and see if you can beat all the current modes.\n",
    "\n",
    "### Code Overview\n",
    "- Dependencies: Python dependencies and loading the spacy model\n",
    "- Project\n",
    "  - Dataset: Download the conversation dataset and parse it into a pytorch Dataset\n",
    "  - Trainer: Trainer function to help with multi-epoch training\n",
    "  - Model 1: Simple Word2Vec + MLP model\n",
    "  - Model 2: Sliding window trigram (Word2Vec)\n",
    "  - Model 3: Embedding bag based model on Trigram\n",
    "- Extensions\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tfdata\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "# python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Fix the random seed so that we get consistent results\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Project\n",
    "✨ Let's Begin ✨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Processing (Common to ALL Solutions)\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "We’ll be using the Empathetic Dialogs dataset open-sourced by Facebook ([link](https://research.fb.com/publications/towards-empathetic-open-domain-conversation-models-a-new-benchmark-and-dataset/)). It can be downloaded as a tar ball from the following [link](https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz)\n",
    "\n",
    "A sample row from the dataset: \n",
    "```\n",
    "conv_id,utterance_idx,context,prompt,speaker_idx,utterance,selfeval,tags\n",
    "hit:12388_conv:24777,1,joyful,I felt overcome with emotions when Christmas came around as a kid,437,Christmas was the best time of year back in the day!,5|5|5_5|5|5, ''\n",
    "```\n",
    "\n",
    "Let's download and explore the dataset and these should automatically get clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dataset and Data loaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html): Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "[LightingDataModule](https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html#datamodules): A datamodule is a shareable, reusable class that encapsulates all the steps needed to process data. A datamodule encapsulates the five steps involved in data processing in PyTorch:\n",
    "\n",
    "1. Download / tokenize / process.\n",
    "2. Clean and (maybe) save to disk.\n",
    "3. Load inside Dataset.\n",
    "4. Apply transforms (rotate, tokenize, etc…).\n",
    "5. Wrap inside a DataLoader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# DATASET #\n",
    "###########\n",
    "\n",
    "class EmpatheticDataset(tfdata.Dataset):\n",
    "    def __init__(self, data_dir: str = 'classification_data', split: str = 'train', transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset_url = 'https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz'\n",
    "        self.directory_name = 'empatheticdialogues'\n",
    "        self.transform = transform\n",
    "        # Check if the dataset directory already exists to avoid re-downloading\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir, exist_ok=True)\n",
    "            # Download the dataset using wget\n",
    "            subprocess.run(['wget', '-q', self.dataset_url, '-O', 'empatheticdialogues.tar.gz'])\n",
    "            # Extract the dataset\n",
    "            subprocess.run(['tar', '-xvf', 'empatheticdialogues.tar.gz', '-C', self.data_dir])\n",
    "            # Remove the tar file to clean up\n",
    "            os.remove('empatheticdialogues.tar.gz')\n",
    "        else:\n",
    "            print(\"Dataset already downloaded and extracted.\")\n",
    "        train_data_url = f\"{self.data_dir}/{self.directory_name}/train.csv\"\n",
    "        val_data_url = f\"{self.data_dir}/{self.directory_name}/valid.csv\"\n",
    "        test_data_url = f\"{self.data_dir}/{self.directory_name}/test.csv\"\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.label_encoder.fit(pd.read_csv(train_data_url, on_bad_lines='skip')['context'])\n",
    "\n",
    "        if split == 'train':\n",
    "            self.pd_data = pd.read_csv(train_data_url, on_bad_lines='skip')\n",
    "        elif split == 'val':\n",
    "            self.pd_data = pd.read_csv(val_data_url, on_bad_lines='skip')\n",
    "        elif split == 'test':\n",
    "            self.pd_data = pd.read_csv(test_data_url, on_bad_lines='skip')\n",
    "        else:\n",
    "            raise ValueError(\"Invalid split. Must be one of 'train', 'val', or 'test'.\")\n",
    "        \n",
    "        # Add a new column for the index of the context\n",
    "        self.pd_data['context_idx'] = self.label_encoder.transform(self.pd_data['context'])\n",
    "\n",
    "        # Apply the transform function to the data\n",
    "        self.data = [self.transform(item) if self.transform else item for _, item in tqdm(list(self.pd_data.iterrows()), \"Applying dataset transform\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's poke around our dataset a little!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:00<00:00, 4780129.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_id</th>\n",
       "      <th>utterance_idx</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>speaker_idx</th>\n",
       "      <th>utterance</th>\n",
       "      <th>selfeval</th>\n",
       "      <th>tags</th>\n",
       "      <th>context_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>1</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>I remember going to see the fireworks with my ...</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>2</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>0</td>\n",
       "      <td>Was this a friend you were in love with_comma_...</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>3</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>This was a best friend. I miss her.</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>4</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>0</td>\n",
       "      <td>Where has she gone?</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>5</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>We no longer talk.</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76663</th>\n",
       "      <td>hit:12424_conv:24848</td>\n",
       "      <td>5</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I found some pictures of my grandma in the att...</td>\n",
       "      <td>389</td>\n",
       "      <td>Yeah reminds me of the good old days.  I miss ...</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76664</th>\n",
       "      <td>hit:12424_conv:24849</td>\n",
       "      <td>1</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>294</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76665</th>\n",
       "      <td>hit:12424_conv:24849</td>\n",
       "      <td>2</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>389</td>\n",
       "      <td>Oh hey that's awesome!  That is awesome right?</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76666</th>\n",
       "      <td>hit:12424_conv:24849</td>\n",
       "      <td>3</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>294</td>\n",
       "      <td>It is soooo awesome.  We have been wanting a b...</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76667</th>\n",
       "      <td>hit:12424_conv:24849</td>\n",
       "      <td>4</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>389</td>\n",
       "      <td>That is awesome!!!! Congratulations!</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76668 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    conv_id  utterance_idx      context  \\\n",
       "0              hit:0_conv:1              1  sentimental   \n",
       "1              hit:0_conv:1              2  sentimental   \n",
       "2              hit:0_conv:1              3  sentimental   \n",
       "3              hit:0_conv:1              4  sentimental   \n",
       "4              hit:0_conv:1              5  sentimental   \n",
       "...                     ...            ...          ...   \n",
       "76663  hit:12424_conv:24848              5  sentimental   \n",
       "76664  hit:12424_conv:24849              1    surprised   \n",
       "76665  hit:12424_conv:24849              2    surprised   \n",
       "76666  hit:12424_conv:24849              3    surprised   \n",
       "76667  hit:12424_conv:24849              4    surprised   \n",
       "\n",
       "                                                  prompt  speaker_idx  \\\n",
       "0      I remember going to the fireworks with my best...            1   \n",
       "1      I remember going to the fireworks with my best...            0   \n",
       "2      I remember going to the fireworks with my best...            1   \n",
       "3      I remember going to the fireworks with my best...            0   \n",
       "4      I remember going to the fireworks with my best...            1   \n",
       "...                                                  ...          ...   \n",
       "76663  I found some pictures of my grandma in the att...          389   \n",
       "76664  I woke up this morning to my wife telling me s...          294   \n",
       "76665  I woke up this morning to my wife telling me s...          389   \n",
       "76666  I woke up this morning to my wife telling me s...          294   \n",
       "76667  I woke up this morning to my wife telling me s...          389   \n",
       "\n",
       "                                               utterance     selfeval tags  \\\n",
       "0      I remember going to see the fireworks with my ...  5|5|5_2|2|5  NaN   \n",
       "1      Was this a friend you were in love with_comma_...  5|5|5_2|2|5  NaN   \n",
       "2                    This was a best friend. I miss her.  5|5|5_2|2|5  NaN   \n",
       "3                                    Where has she gone?  5|5|5_2|2|5  NaN   \n",
       "4                                     We no longer talk.  5|5|5_2|2|5  NaN   \n",
       "...                                                  ...          ...  ...   \n",
       "76663  Yeah reminds me of the good old days.  I miss ...  5|5|5_5|5|5  NaN   \n",
       "76664  I woke up this morning to my wife telling me s...  5|5|5_5|5|5  NaN   \n",
       "76665     Oh hey that's awesome!  That is awesome right?  5|5|5_5|5|5  NaN   \n",
       "76666  It is soooo awesome.  We have been wanting a b...  5|5|5_5|5|5  NaN   \n",
       "76667               That is awesome!!!! Congratulations!  5|5|5_5|5|5  NaN   \n",
       "\n",
       "       context_idx  \n",
       "0               28  \n",
       "1               28  \n",
       "2               28  \n",
       "3               28  \n",
       "4               28  \n",
       "...            ...  \n",
       "76663           28  \n",
       "76664           29  \n",
       "76665           29  \n",
       "76666           29  \n",
       "76667           29  \n",
       "\n",
       "[76668 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the internal Pandas DataFrame within the EmpatheticDataset\n",
    "sample_dataset = EmpatheticDataset()\n",
    "sample_dataset.pd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the label encoder in the data module.  It should be able to convert the string labels to integers and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoder classes: ['afraid' 'angry' 'annoyed' 'anticipating' 'anxious' 'apprehensive'\n",
      " 'ashamed' 'caring' 'confident' 'content' 'devastated' 'disappointed'\n",
      " 'disgusted' 'embarrassed' 'excited' 'faithful' 'furious' 'grateful'\n",
      " 'guilty' 'hopeful' 'impressed' 'jealous' 'joyful' 'lonely' 'nostalgic'\n",
      " 'prepared' 'proud' 'sad' 'sentimental' 'surprised' 'terrified' 'trusting']\n",
      "Label for \"sad\": [27]\n",
      "Label for \"hopeful\": [19]\n",
      "Label for \"angry\": [1]\n"
     ]
    }
   ],
   "source": [
    "print(f'Label encoder classes: {sample_dataset.label_encoder.classes_}')\n",
    "print(f'Label for \"sad\": {sample_dataset.label_encoder.transform([\"sad\"])}')\n",
    "print(f'Label for \"hopeful\": {sample_dataset.label_encoder.transform([\"hopeful\"])}')\n",
    "print(f'Label for \"angry\": {sample_dataset.label_encoder.transform([\"angry\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "Now, let's create a transform to extract the context and the utterance from the dataset.\n",
    "\n",
    "The columns we care about are:\n",
    "1. \"context\": This is the emotion we're trying to predict (this has already been converted to a number usign the dataset label encoder)\n",
    "1. \"prompt\" and \"utterance\": We'll combine these sentences and use them as input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_mean_vectors(item):\n",
    "    # Combine 'prompt' and 'utterance' into a single string\n",
    "    input_string = item['prompt'] + ' ' + item['utterance']\n",
    "    # Vectorize the input string\n",
    "    x = np.mean([token.vector for token in nlp.make_doc(input_string)], axis=0)\n",
    "    # Retrieve the context label\n",
    "    y = item['context_idx'] # This column was added in the EmpatheticDataset class __init__ method\n",
    "    return {\n",
    "        'input_string': input_string, # This is useful for visualization/debugging\n",
    "        'sentiment': item['context'], # This is useful for visualization/debugging\n",
    "        'input_vector': x,\n",
    "        'sentiment_idx': y\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate\n",
    "Next, we'll create a collate function that tokenizes the batch and returns the tokenized input and the emotion label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # A torch data loader will combine a list of samples into a batch\n",
    "    # This function will be used to process the batch\n",
    "    # batch is a list of the outputs from the __getitem__ method of the EmpatheticDataset class\n",
    "    # Separate the batch into input and target, then convert to tensors\n",
    "    input_tensors = torch.stack([torch.tensor(item['input_vector']) for item in batch])\n",
    "    target_tensors = torch.stack([torch.tensor(item['sentiment_idx']) for item in batch])\n",
    "    return {\n",
    "        'input': input_tensors,\n",
    "        'target': target_tensors,\n",
    "        'input_string': [item['input_string'] for item in batch], # This is useful for visualization/debugging\n",
    "        'sentiment': [item['sentiment'] for item in batch] # This is useful for visualization/debugging\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmpatheticDialoguesDataModule(L.LightningDataModule):\n",
    "    def __init__(self, batch_size=32, collate_fn=None, transform=None):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.collate_fn = collate_fn\n",
    "        self.transform = transform\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # This downloads the dataset and prepares it\n",
    "        # It doesn't save anything to the data module, just prepares the dataset\n",
    "        EmpatheticDataset()\n",
    "        self.label_encoder = EmpatheticDataset().label_encoder\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # Retrieve the dataset from disk\n",
    "        self.train_dataset = EmpatheticDataset(split='train', transform=self.transform)\n",
    "        self.val_dataset = EmpatheticDataset(split='val', transform=self.transform)\n",
    "        self.test_dataset = EmpatheticDataset(split='test', transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return tfdata.DataLoader(self.train_dataset,\n",
    "                               batch_size=self.batch_size,\n",
    "                               collate_fn=self.collate_fn,\n",
    "                               num_workers=0,\n",
    "                               shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return tfdata.DataLoader(self.val_dataset,\n",
    "                               batch_size=self.batch_size,\n",
    "                               collate_fn=self.collate_fn,\n",
    "                               num_workers=0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return tfdata.DataLoader(self.test_dataset,\n",
    "                               batch_size=self.batch_size,\n",
    "                               collate_fn=self.collate_fn,\n",
    "                               num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classfier Module\n",
    "\n",
    "We've now created the DataLoader and Datasets we'll use in the entire project, it is time to write the training and testing code via a `LightningModule`. \n",
    "\n",
    "[LightingModule](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html): organizes your PyTorch code into 5 sections\n",
    "\n",
    "1. Computations (init).\n",
    "2. Train loop (training_step)\n",
    "3. Validation loop (validation_step)\n",
    "4. Test loop (test_step)\n",
    "5. Optimizers (configure_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier(L.LightningModule):\n",
    "  def __init__(self, model, batch_size, learning_rate, num_classes):\n",
    "      super().__init__()\n",
    "      self.model = model\n",
    "      self.batch_size = batch_size\n",
    "      self.learning_rate = learning_rate\n",
    "      self.accuracy = MulticlassAccuracy(num_classes=num_classes)\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x = batch[\"input\"]\n",
    "    y = batch[\"target\"]\n",
    "    y_hat = self.model(x)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "    self.log_dict(\n",
    "      {'train_loss': loss},\n",
    "      batch_size=self.batch_size,\n",
    "      prog_bar=True\n",
    "    )\n",
    "    return loss\n",
    "  \n",
    "  def validation_step(self, batch, batch_nb):\n",
    "    x = batch[\"input\"]\n",
    "    y = batch[\"target\"]\n",
    "    y_hat = self.model(x)\n",
    "    val_loss = F.cross_entropy(y_hat, y)\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    self.log_dict(\n",
    "        {\n",
    "          'val_loss': val_loss,\n",
    "          'val_accuracy': self.accuracy(predictions, y)\n",
    "        },\n",
    "        batch_size=self.batch_size,\n",
    "        prog_bar=True\n",
    "      )\n",
    "    return val_loss\n",
    "\n",
    "  def test_step(self, batch, batch_nb):\n",
    "    x = batch[\"input\"]\n",
    "    y = batch[\"target\"]\n",
    "    y_hat = self.model(x)\n",
    "    test_loss = F.cross_entropy(y_hat, y)\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    self.log_dict(\n",
    "        {\n",
    "          'test_loss': test_loss,\n",
    "          'test_accuracy': self.accuracy(predictions, y)\n",
    "        },\n",
    "        batch_size=self.batch_size,\n",
    "        prog_bar=True\n",
    "      )\n",
    "    return test_loss\n",
    "  \n",
    "  def predict_step(self, batch, batch_idx):\n",
    "    y_hat = self.model(batch[\"input\"])\n",
    "    predictions = torch.argmax(y_hat, dim=1)\n",
    "    return {'logits':y_hat, 'predictions': predictions, 'sentiment_labels': batch[\"sentiment\"], 'input_string': batch['input_string']}\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Are we building models yet? Finally the time has come to build our baseline model and then we'll work towards improving it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Average word vector of the sentence -- Baseline\n",
    "##### <font color='red'>Expected accuracy: ~29 - 32%</font>\n",
    "\n",
    "Let's build our first simple word2vec based model we'll use as our baseline.\n",
    "\n",
    "Here we've three key pieces:\n",
    "\n",
    "1. *WordVectorClassificationModel*: Simple linear model that just has one single neuron layer that maps the input word2vec dimensions (300) to the output classes (32) building a really simple classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVectorClassificationModel(torch.nn.Module):\n",
    "  def __init__(self, word_vec_dimension, num_classes):\n",
    "    super().__init__()\n",
    "    self.linear_layer = torch.nn.Linear(word_vec_dimension, num_classes)\n",
    "\n",
    "  # 🌟🌟🌟 Pay extra attention here since you'll have to work on this in the models 🌟🌟🌟\n",
    "  def forward(self, batch):\n",
    "    \"\"\"Projection from word_vec_dim to n_classes\n",
    "\n",
    "    Batch is of shape (batch_size, max_seq_len, word_vector_dim)\n",
    "    \"\"\"\n",
    "    return self.linear_layer(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "Now, let's use the Lighning [`Trainer`](https://lightning.ai/docs/pytorch/latest/common/trainer.html) to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    transform,\n",
    "    collate_fn,\n",
    "    batch_size=32,\n",
    "    max_epochs=4,\n",
    "    learning_rate=0.001,\n",
    "):\n",
    "    # Create a pytorch trainer\n",
    "    trainer = L.Trainer(max_epochs=max_epochs, check_val_every_n_epoch=1)\n",
    "\n",
    "    # Initialize our data loader with the passed vectorizer\n",
    "    data_module = EmpatheticDialoguesDataModule(collate_fn=collate_fn,\n",
    "                                                transform=transform,\n",
    "                                                batch_size=batch_size)\n",
    "    data_module.prepare_data()\n",
    "    data_module.setup('fit')\n",
    "\n",
    "    # Instantiate a new lightning module\n",
    "    module = EmotionClassifier(model,\n",
    "                            batch_size=batch_size,\n",
    "                            learning_rate=learning_rate,\n",
    "                            num_classes=len(data_module.label_encoder.classes_))\n",
    "\n",
    "    # Train and validate the model\n",
    "    trainer.fit(module, data_module.train_dataloader(), val_dataloaders=data_module.val_dataloader())\n",
    "\n",
    "    # Test the model\n",
    "    trainer.test(module, data_module.test_dataloader())\n",
    "\n",
    "    # Predict on the same test set to show some output\n",
    "    output = trainer.predict(module, data_module.test_dataloader())\n",
    "\n",
    "    # Un-collate the output\n",
    "    uncollated_output = {}\n",
    "    for batch_output in output:\n",
    "        for k, v in batch_output.items():\n",
    "            if k not in uncollated_output:\n",
    "                uncollated_output[k] = []\n",
    "            uncollated_output[k].extend(v)\n",
    "\n",
    "    return uncollated_output, data_module.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/grinch/Developer/nlp_course/nlp_course_env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:00<00:00, 6503304.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:00<00:00, 6568126.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:10<00:00, 7290.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 6318/6318 [00:00<00:00, 6533.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 5701/5701 [00:00<00:00, 6136.24it/s]\n",
      "\n",
      "  | Name     | Type                          | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model    | WordVectorClassificationModel | 9.6 K \n",
      "1 | accuracy | MulticlassAccuracy            | 0     \n",
      "-----------------------------------------------------------\n",
      "9.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.6 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 300/300 [00:02<00:00, 121.16it/s, v_num=46, train_loss=2.270, val_loss=2.330, val_accuracy=0.331]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 300/300 [00:02<00:00, 121.03it/s, v_num=46, train_loss=2.270, val_loss=2.330, val_accuracy=0.331]\n",
      "Testing DataLoader 0: 100%|██████████| 23/23 [00:01<00:00, 11.99it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.32079431414604187\n",
      "        test_loss           2.3128020763397217\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Predicting DataLoader 0: 100%|██████████| 23/23 [00:00<00:00, 172.02it/s]\n",
      "Input: hopeful: I know things will work out in God's perfect timing Yes I am. For everything we want for the baby to be gifted to us lol\n",
      "Predicted label: ['hopeful']\n",
      "Actual label: hopeful\n",
      "\n",
      "Input: I lost my temper with my daughter the other day and was a bit ill with her. I felt so bad that I had raised my voice and hurt her feelings. My daughter caught me in a bad mood the other day and was being a bit argumentative. I lost my temper_comma_ yelled and really hurt her feelings. I felt bad that it happened.\n",
      "Predicted label: ['guilty']\n",
      "Actual label: ashamed\n",
      "\n",
      "Input: While I didn't know him personally_comma_ I was quite distressed to hear of the passing of Anthony Bourdain. That a man with such talent who has given so much joy to so many people_comma_ should feel he has to take his life is just soul-destroying. It is. I read his book Kitchen Confidential. He's been battling addiction and demons all his life. I thought he'd found peace_comma_ but I guess not.\n",
      "Predicted label: ['impressed']\n",
      "Actual label: sad\n",
      "\n",
      "Input: Well_comma_ I've finally found my perfect restaurant/grocery store. I stopped by the other day and bought some food and couldn't have been more pleased with everything there! They're the same shape as regular green beans_comma_ but lighter in color and they have red streaks through them. You treat them as you would green beans.\n",
      "Predicted label: ['disgusted']\n",
      "Actual label: content\n",
      "\n",
      "Input: My husband's grandfather just passed away recently_comma_ and his three sons are acting very childish and selfish about everything concerning the will. They even had arguments about who was getting what_comma_ in front of him_comma_ while he was still alive. The whole thing makes me so angry. My husband's grandfather recently passed away_comma_ and his three sons are acting very childish and selfish about everything concerning the will.\n",
      "Predicted label: ['impressed']\n",
      "Actual label: furious\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uncollated_output, label_encoder = train(\n",
    "    model=WordVectorClassificationModel(word_vec_dimension=300, num_classes=32),\n",
    "    transform=transform_mean_vectors,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=256,\n",
    "    max_epochs=4,\n",
    "    learning_rate=0.001,\n",
    ")\n",
    "\n",
    "# Feel free to explore the uncollated_output dictionary to see the predictions\n",
    "for _ in range(5):\n",
    "    # Randomly select a sample from the test set\n",
    "    i = np.random.randint(len(uncollated_output['input_string']))\n",
    "    print(f'Input: {uncollated_output[\"input_string\"][i]}')\n",
    "    print(f'Predicted label: {label_encoder.inverse_transform([uncollated_output[\"predictions\"][i].item()])}')\n",
    "    print(f'Actual label: {uncollated_output[\"sentiment_labels\"][i]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉🎉🎉 WE HAVE OUR TEXT CLASSIFIER 🎉🎉🎉\n",
    "\n",
    "Now might be a good time to play around with the vectorizer and the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Part: 1 - Model 2: Sliding Window of Vectors ---- TO BE COMPLETED\n",
    "##### <font color='red'>Expected accuracy: ~30 to 36%</font>\n",
    "\n",
    "We'll be re-using the simple linear model from Model-1 but changing the input to use sliding windows instead of one word at a time.\n",
    "\n",
    "Implement a new `transform_sliding_window` which is a variant of the `transform_mean_vectors` that operates. Here are some instructions on how to implement it.\n",
    "\n",
    "1. Split the sentence into chunks of the size of the n_grams parameter.\n",
    "2. Concat all the spacy embeddings of the tokens inside to create embeddings of the chuck. Each chunk vector is of the size of `n_gram * size_of_embedding`\n",
    "3. Sentence vector is the average of all chunk vectors.\n",
    "4. Return the sentence_vector and tokens (for debugging, option: could just return None for tokens)\n",
    "\n",
    "Does this model perform better than our baseline? Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sliding_window(item):\n",
    "    n_grams: int = 3  ## ADAPT, Change it to your liking\n",
    "\n",
    "    # Combine 'prompt' and 'utterance' into a single string\n",
    "    input_string = item['prompt'] + ' ' + item['utterance']\n",
    "    \"\"\"Given a sentence, tokenize it and calculate a vector for that sentence.\n",
    "\n",
    "    Sentence is of length (n)\n",
    "\n",
    "    1. Split the sentence into tokens using Spacy's function make_doc\n",
    "    2. Split the list of token into size of the n_grams parameter.\n",
    "    3. Concat all the spacy embeddings of the tokens inside to create embeddings of the chuck. Each chunk vector is of the size of n_grams * size_of_embedding\n",
    "    4. Sentence vector is the average of all chunk vectors.\n",
    "    5. Return the sentence_vector and tokens (option: could just return None for tokens)\n",
    "\n",
    "    Sentence_vector is of length (n_grams * word_vector_dim)\n",
    "    Sentence_tokens is of length (n)\n",
    "\n",
    "    Example of word tri-gram encoding: \"I am doing great right now.\":\n",
    "      <EMPTY (300,)> <EMPTY (300,)> <I (300,)> -> (900, )\n",
    "      <EMPTY (300,)> <I (300,)> <am (300,)> -> (900, )\n",
    "      <I (300,)> <am (300,)> <doing (300,)> -> (900, )\n",
    "      <am (300,)> <doing (300,)> <great (300,)> -> (900, )\n",
    "      <doing (300,)> <great (300,)> <right (300,)> -> (900, )\n",
    "      <great (300,)> <right (300,)> <now (300,)> -> (900, )\n",
    "      <right (300,)> <now (300,)> <EMPTY (300,)> -> (900, )\n",
    "      <now (300,)> <EMPTY (300,)> <EMPTY (300,)> -> (900, )\n",
    "\n",
    "    We'd encourage you to also try other variants to encode!\n",
    "    \"\"\"\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    sentence_vectors = [token.vector for token in nlp.make_doc(input_string)]\n",
    "    # pad the sentence_vectors with (n_gram-1) empty vectors on both sides\n",
    "    sentence_vectors = [np.zeros_like(sentence_vectors[0])]*(n_grams-1) + sentence_vectors + [np.zeros_like(sentence_vectors[0])]*(n_grams-1)\n",
    "    # create the n-gram chunks\n",
    "    split_sentence = []\n",
    "    for i in range(len(sentence_vectors) - n_grams + 1):\n",
    "        split_sentence.append(np.concatenate(sentence_vectors[i:i+n_grams], axis=0))\n",
    "    # calculate the sentence vector\n",
    "    x = np.mean(split_sentence, axis=0)\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    # Retrieve the context label\n",
    "    y = item['context_idx'] # This column was added in the EmpatheticDataset class __init__ method\n",
    "    return {\n",
    "        'input_string': input_string, # This is useful for visualization/debugging\n",
    "        'sentiment': item['context'], # This is useful for visualization/debugging\n",
    "        'input_vector': x,\n",
    "        'sentiment_idx': y\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:00<00:00, 6125588.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:00<00:00, 6416747.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:13<00:00, 5665.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 6318/6318 [00:01<00:00, 5005.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 5701/5701 [00:01<00:00, 4543.41it/s]\n",
      "\n",
      "  | Name     | Type                          | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model    | WordVectorClassificationModel | 28.8 K\n",
      "1 | accuracy | MulticlassAccuracy            | 0     \n",
      "-----------------------------------------------------------\n",
      "28.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "28.8 K    Total params\n",
      "0.115     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 300/300 [00:02<00:00, 120.33it/s, v_num=47, train_loss=2.020, val_loss=2.230, val_accuracy=0.351]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 300/300 [00:02<00:00, 120.15it/s, v_num=47, train_loss=2.020, val_loss=2.230, val_accuracy=0.351]\n",
      "Testing DataLoader 0: 100%|██████████| 23/23 [00:00<00:00, 78.38it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.34902888536453247\n",
      "        test_loss           2.2372679710388184\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Predicting DataLoader 0: 100%|██████████| 23/23 [00:00<00:00, 224.64it/s]\n",
      "Input: I told my brother a secret and I hope he keeps it! I told my brother that I was planning to propose to my best friend_comma_ but I hope he keeps it a secret!\n",
      "Predicted label: ['trusting']\n",
      "Actual label: trusting\n",
      "\n",
      "Input: Playing with puppies. It was definitely exciting for me! I haven't been able to play with a dog since my own passed away.\n",
      "Predicted label: ['content']\n",
      "Actual label: excited\n",
      "\n",
      "Input: I have an amazing husband who is always there for me no matter what.  He makes me feel so loved and I can't imagine life without him. I have an amazing husband who is always there for me no matter what\n",
      "Predicted label: ['grateful']\n",
      "Actual label: content\n",
      "\n",
      "Input: I had just spend 100 dollars on rolling a slot machine. After I just left and didn't win anything_comma_ someone came up and hit triple 7s at the machine i was just at. I was just at the casino_comma_ spent almost 100 dollars on a single machine. I didn't will anything.\n",
      "Predicted label: ['devastated']\n",
      "Actual label: jealous\n",
      "\n",
      "Input: yes i am going to coffee shop the man speech is  so guilty thank you\n",
      "Predicted label: ['grateful']\n",
      "Actual label: guilty\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uncollated_output, label_encoder = train(\n",
    "    model=WordVectorClassificationModel(word_vec_dimension=300*3, num_classes=32),\n",
    "    transform=transform_sliding_window,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=256,\n",
    "    max_epochs=4,\n",
    "    learning_rate=0.001,\n",
    ")\n",
    "\n",
    "# Feel free to explore the uncollated_output dictionary to see the predictions\n",
    "for _ in range(5):\n",
    "    # Randomly select a sample from the test set\n",
    "    i = np.random.randint(len(uncollated_output['input_string']))\n",
    "    print(f'Input: {uncollated_output[\"input_string\"][i]}')\n",
    "    print(f'Predicted label: {label_encoder.inverse_transform([uncollated_output[\"predictions\"][i].item()])}')\n",
    "    print(f'Actual label: {uncollated_output[\"sentiment_labels\"][i]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Part: 2 - Model 3: EmbeddingBag  ---- TO BE COMPLETED\n",
    "##### <font color='red'>Expected accuracy: ~32 to 38%</font>\n",
    "\n",
    "The third model we're going to build is an embedding layer based model. Here instead of using pre-trained word-embeddings we'll be creating new vectors as part of the training process. How do you think this model will perform?\n",
    "\n",
    "Implementation has the following steps:\n",
    "\n",
    "1. **`get_char_trigram_token_map`**: Create a map of trigram to trigram_id for the most common trigrams in the training sentences.  Here are some steps that should help with the implementation.\n",
    "      1. Compute a frequency map of the `num_tokens` most common  character trigrams in the training data. **Note: A trigram is a group of three characters.  The model won't understand words- don't use the Spacy tokenizer for this!**.\n",
    "      3. Create unique integer ids for all these trigrams 1...N (don't use 0 because that is used for padding!)\n",
    "\n",
    "2. **Vectorizer**: Implement a new transform for each sentence that does the following:\n",
    "    1. Get all trigrams for the sentence\n",
    "    2. Get id for every trigram\n",
    "    4. Append all the ids into a list and that is your input sentence vector\n",
    "\n",
    "3. **Forward pass of the model**: Implement the forward pass of the model\n",
    "    1. Pass the input batch through the embedding layer\n",
    "    1. Pass the output of the embedding layer into our linear layer  \n",
    "\n",
    "Note:  We have provided a new `collate_fn` because the input to the model is now a padded list of integers.\n",
    "\n",
    "Some rational for trying this out is:\n",
    "- We average the embeddings in a sentence anyway so word or not word maybe doesn't matter.\n",
    "- Vocabalary of character trigrams is much smaller than word trigrams so our models are easier to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_char_trigram_token_map(train_data, num_tokens, verbose=True):\n",
    "    \"\"\"\n",
    "    1. Compute a frequency map of the `num_tokens` most common trigrams in the training data.\n",
    "    2. Create unique integer ids for all these tokens 1...N\n",
    "\n",
    "    We HIGHLY recommend using the `collections.Counter` class to compute the frequency map.\n",
    "\n",
    "    Args:\n",
    "    train_data: List of strings from the training corpus\n",
    "    \"\"\"\n",
    "    token_to_id_map = {}\n",
    "\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    def iterate_trigrams(sentences):\n",
    "        if verbose:\n",
    "            sentences = tqdm(sentences, \"Computing trigrams\")\n",
    "        for sentence in sentences:\n",
    "            for i in range(len(sentence) - 2):\n",
    "                yield sentence[i:i+3]\n",
    "    counter = Counter(iterate_trigrams(train_data))\n",
    "    token_to_id_map = {k: i+1 for i, (k, _) in enumerate(counter.most_common(num_tokens))}\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "\n",
    "    return token_to_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing trigrams: 100%|██████████| 1/1 [00:00<00:00, 7570.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'he ': 1,\n",
       " 'The': 2,\n",
       " 'e q': 3,\n",
       " ' qu': 4,\n",
       " 'qui': 5,\n",
       " 'uic': 6,\n",
       " 'ick': 7,\n",
       " 'ck ': 8,\n",
       " 'k b': 9,\n",
       " ' br': 10}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_char_trigram_token_map([\"The quick brown fox jumps over the lazy dog\"], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just validate the output of the tokenizer before we train the model. We should see something like:\n",
    "\n",
    "```\n",
    "[(' th', 1), (' I ', 2), ('the', 3), (' to', 4), ('ing', 5)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:00<00:00, 386821.42it/s]\n",
      "Computing trigrams: 100%|██████████| 76668/76668 [00:01<00:00, 54422.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' th', 1), (' I ', 2), ('the', 3), (' to', 4), ('ing', 5)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_map = get_char_trigram_token_map(EmpatheticDataset(transform=lambda item: item['prompt'] + ' ' + item['utterance']), 5000)\n",
    "list(token_map.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_trigram_token_transform(token_map):\n",
    "    \"\"\" Returns a transform function that converts a string to a list of token ids using the token_map.\n",
    "     \n",
    "    This effectively saves the token_map into the transform function.\n",
    "    \"\"\"\n",
    "    def transform_trigram_tokenize(item):\n",
    "        # Combine 'prompt' and 'utterance' into a single string\n",
    "        input_string = item['prompt'] + ' ' + item['utterance']\n",
    "        # Calculate the tokenized input\n",
    "        \"\"\"\n",
    "        Given a inputs sentence (input_string), do the following -\n",
    "        1. Get all trigrams for the sentence\n",
    "        2. Get id for every trigram (that exists)\n",
    "        3. Append all the ids into a list and that is your sentence vector\n",
    "        \"\"\"\n",
    "        ### TO BE IMPLEMENTED ###\n",
    "        def iterate_trigrams(sentence):\n",
    "            for i in range(len(sentence) - 2):\n",
    "                yield sentence[i:i+3]\n",
    "        x = [token_map[trigram] for trigram in iterate_trigrams(input_string) if trigram in token_map]\n",
    "        ### TO BE IMPLEMENTED ###\n",
    "        # Retrieve the context label\n",
    "        y = item['context_idx'] # This column was added in the EmpatheticDataset class __init__ method\n",
    "        return {\n",
    "            'input_string': input_string, # This is useful for visualization/debugging\n",
    "            'sentiment': item['context'], # This is useful for visualization/debugging\n",
    "            'input_tokens': x,\n",
    "            'sentiment_idx': y\n",
    "        }\n",
    "    return transform_trigram_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a new collate function that prepares the input as a padded tensor of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_tokens(batch):\n",
    "    # A torch data loader will combine a list of samples into a batch\n",
    "    # This function will be used to process the batch\n",
    "    # batch is a list of the outputs from the __getitem__ method of the EmpatheticDataset class\n",
    "    # Separate the batch into input and target, then convert to tensors\n",
    "    PAD_TOKEN = 0\n",
    "    input_tensors = pad_sequence([torch.tensor(item['input_tokens'], dtype=torch.long) for item in batch],\n",
    "                                 batch_first=True,\n",
    "                                 padding_value=PAD_TOKEN)\n",
    "    target_tensors = torch.stack([torch.tensor(item['sentiment_idx']) for item in batch])\n",
    "    return {\n",
    "        'input': input_tensors,\n",
    "        'target': target_tensors,\n",
    "        'input_string': [item['input_string'] for item in batch], # This is useful for visualization/debugging\n",
    "        'sentiment': [item['sentiment'] for item in batch] # This is useful for visualization/debugging\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the simple embedding layer based model and start training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBagClassificationModel(torch.nn.Module):\n",
    "  def __init__(self, num_tokens, embed_dim, n_classes):\n",
    "    super().__init__()\n",
    "    self.classes = n_classes\n",
    "    # self.embedding = torch.nn.EmbeddingBag(num_tokens, embed_dim)\n",
    "    self.embedding = torch.nn.Embedding(num_tokens, embed_dim, padding_idx=0)\n",
    "    self.linear_layer = torch.nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "  def forward(self, batch):\n",
    "    \"\"\"Pass the input batch through the embedding layer and then follow it up with the linear layer\n",
    "    \"\"\"\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    y = self.embedding(batch)\n",
    "    y = torch.mean(y, dim=1)\n",
    "    y = self.linear_layer(y)\n",
    "    ### TO BE IMPLEMENTED ###\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:00<00:00, 373074.87it/s]\n",
      "Computing trigrams: 100%|██████████| 76668/76668 [00:01<00:00, 47881.59it/s]\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:00<00:00, 6424953.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:00<00:00, 6775577.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 76668/76668 [00:01<00:00, 38870.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 6318/6318 [00:00<00:00, 34560.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying dataset transform: 100%|██████████| 5701/5701 [00:00<00:00, 31902.12it/s]\n",
      "\n",
      "  | Name     | Type                            | Params\n",
      "-------------------------------------------------------------\n",
      "0 | model    | EmbeddingBagClassificationModel | 160 K \n",
      "1 | accuracy | MulticlassAccuracy              | 0     \n",
      "-------------------------------------------------------------\n",
      "160 K     Trainable params\n",
      "0         Non-trainable params\n",
      "160 K     Total params\n",
      "0.641     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 396/1198 [01:44<03:31,  3.79it/s, v_num=50, train_loss=2.910, val_loss=2.880, val_accuracy=0.165] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/grinch/Developer/nlp_course/nlp_course_env/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 90/90 [00:07<00:00, 11.85it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy          0.178696870803833\n",
      "        test_loss            2.736837148666382\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Predicting DataLoader 0: 100%|██████████| 90/90 [00:07<00:00, 11.72it/s] \n",
      "Input: I saw a homeless lady  I saw a homeless lady and i felt so bad for her because it was so hot outside so went to five guys and bought her some food and cold drinks\n",
      "Predicted label: ['disgusted']\n",
      "Actual label: caring\n",
      "\n",
      "Input: My husband usually gets a large bonus each year around this time_comma_ but there's no guarantee. It's hard waiting_comma_ and not knowing! He is a welder and iron worker_comma_ so he works very hard all year long and does extensive amounts of travel. He has earned it!\n",
      "Predicted label: ['proud']\n",
      "Actual label: anxious\n",
      "\n",
      "Input: I'm annoyed today is my off day and I wanted to sleep in_comma_ yet here I am awake early. It sucks. I guess my body doesn't want me to sleep in any more.\n",
      "Predicted label: ['sad']\n",
      "Actual label: annoyed\n",
      "\n",
      "Input: There was a neighbor I never had met that called my son a derogatory name one time when he was riding his bike. She didn't like that he got in her yard a little bit when he went around the corner and was very ugly to him_comma_ and he was only 10 years old at the time.  There was a back neighbor that called my 10 year old son a ugly name one time_comma_ simply because he entered her yard slightly when he rode around the corner.!\n",
      "Predicted label: ['impressed']\n",
      "Actual label: furious\n",
      "\n",
      "Input: A customer is refusing to pay a bill because they claim it has already been paid but cannot provide any proof of doing so. We are a retail outfit. So we have some big customers that pay on established terms.\n",
      "Predicted label: ['surprised']\n",
      "Actual label: angry\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_tokens = 5000\n",
    "\n",
    "token_map = get_char_trigram_token_map(EmpatheticDataset(transform=lambda item: item['prompt'] + ' ' + item['utterance']), num_tokens)\n",
    "\n",
    "uncollated_output, label_encoder = train(\n",
    "    model=EmbeddingBagClassificationModel(num_tokens=num_tokens+2, embed_dim=300, n_classes=32),\n",
    "    transform=get_char_trigram_token_transform(token_map),\n",
    "    collate_fn=collate_fn_tokens,\n",
    "    batch_size=256,\n",
    "    max_epochs=10,\n",
    "    learning_rate=0.001,\n",
    ")\n",
    "\n",
    "# Feel free to explore the uncollated_output dictionary to see the predictions\n",
    "for _ in range(5):\n",
    "    # Randomly select a sample from the test set\n",
    "    i = np.random.randint(len(uncollated_output['input_string']))\n",
    "    print(f'Input: {uncollated_output[\"input_string\"][i]}')\n",
    "    print(f'Predicted label: {label_encoder.inverse_transform([uncollated_output[\"predictions\"][i].item()])}')\n",
    "    print(f'Actual label: {uncollated_output[\"sentiment_labels\"][i]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉 CONGRATS!!! on finishing the assignment. Now is a good time to pause and reflect how much progress we've made in understanding word vectors, reading some pytorch code and build our first model. But hey, don't stop here, there is a lot to do or play with in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "\n",
    "Now that you've worked through the project. There is a lot more for us to try.\n",
    "\n",
    "- Which model performed the best? Why do you think that was?\n",
    "- Try decreasing and increasing the size of the dataset. How does that impact training time and accuracy of each model?\n",
    "- Try adding a hidden layer to the baseline of the models and see if that changes anything\n",
    "- Does adding a hidden layer to the embedding bag model help?\n",
    "- visualize a [confusion matrix](https://torchmetrics.readthedocs.io/en/latest/references/functional.html#confusion-matrix-func) of N*N of actual class vs predicted class (N = number of classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
