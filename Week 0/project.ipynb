{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DUPLICATE THIS COLAB TO START WORKING ON IT. Using File > Save a copy to drive.\n",
    "\n",
    "# Prereq Week: Text Classification\n",
    "\n",
    "### What are we building\n",
    "We’ll continue to apply our learning philosophy of repetition as we build multiple classification models of increasing complexity in the following order:\n",
    "\n",
    "1. Average of Word2Vec + MLP Layer\n",
    "1. Can we concatenate 3 token embeddings and then average them? Does this do better than the previous method?\n",
    "1. Build an embedding layer based model.\n",
    "1. **Extension**: Explore different parameters, features and architectures. \n",
    "\n",
    "###  Evaluation\n",
    "We’ll be evaluating our models on the following metric: \n",
    "\n",
    "1. Accuracy: is the ratio of the number of correctly classified instances to the total number of instances\n",
    "1. **Extension**: this is a multi-class classification problem, visualize a [confusion matrix](https://torchmetrics.readthedocs.io/en/latest/references/functional.html#confusion-matrix-func) of N*N of actual class vs predicted class (N = number of classes).\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. We've provide scaffolding for all the boiler plate PyTorch code to get to our first model. This covers downloading and parsing the dataset, training code for the baseline model. **Make sure to read all the steps and internalize what is happening**.\n",
    "1. At this point our model gets to an accuracy of about 0.32. After this we'll try to improve the model by using sliding windows of text instead of just one word at a time. **Does this improve accuracy?**\n",
    "1. The third model we're going to build is an embedding layer based model. Here instead of using pre-trained word-embeddings we'll be creating new vectors as part of the training process. **How do you think this model will perform?**\n",
    "1. **Extension**: We've suggested a bunch of extensions to the project so go crazy, tweak any parts of the pipeline and see if you can beat all the current modes.\n",
    "\n",
    "### Code Overview\n",
    "- Dependencies: Python dependencies and loading the spacy model\n",
    "- Project\n",
    "  - Dataset: Download the conversation dataset and parse it into a pytorch Dataset\n",
    "  - Trainer: Trainer function to help with multi-epoch training\n",
    "  - Model 1: Simple Word2Vec + MLP model\n",
    "  - Model 2: Sliding window trigram (Word2Vec)\n",
    "  - Model 3: Embedding bag based model on Trigram\n",
    "- Extensions\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from collections import Counter\n",
    "import en_core_web_lg\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "# python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Fix the random seed so that we get consistent results\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Project\n",
    "✨ Let's Begin ✨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Processing (Common to ALL Solutions)\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "We’ll be using the Empathetic Dialogs dataset open-sourced by Facebook ([link](https://research.fb.com/publications/towards-empathetic-open-domain-conversation-models-a-new-benchmark-and-dataset/)). It can be downloaded as a tar ball from the following [link](https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz)\n",
    "\n",
    "A sample row from the dataset: \n",
    "```\n",
    "conv_id,utterance_idx,context,prompt,speaker_idx,utterance,selfeval,tags\n",
    "hit:12388_conv:24777,1,joyful,I felt overcome with emotions when Christmas came around as a kid,437,Christmas was the best time of year back in the day!,5|5|5_5|5|5, ''\n",
    "```\n",
    "\n",
    "The three columns we'll primarily focus on are:\n",
    "1. context ==> emotion we're trying to predict\n",
    "1. prompt + utterance ==> We'll combine these sentences and use them as input \n",
    "\n",
    "But let's download and explore the dataset and these should automatically get clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-14 11:11:18--  https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 99.84.238.181, 99.84.238.162, 99.84.238.206, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|99.84.238.181|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 28022709 (27M) [application/gzip]\n",
      "Saving to: ‘empatheticdialogues.tar.gz.1’\n",
      "\n",
      "empatheticdialogues 100%[===================>]  26.72M  9.97MB/s    in 2.7s    \n",
      "\n",
      "2024-02-14 11:11:21 (9.97 MB/s) - ‘empatheticdialogues.tar.gz.1’ saved [28022709/28022709]\n",
      "\n",
      "x empatheticdialogues/\n",
      "x empatheticdialogues/test.csv\n",
      "x empatheticdialogues/train.csv\n",
      "x empatheticdialogues/valid.csv\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY_NAME=\"classification_data\"\n",
    "TRAIN_FILE=\"classification/empatheticdialogues/train.csv\"\n",
    "VALIDATION_FILE=\"classification/empatheticdialogues/valid.csv\"\n",
    "TEST_FILE=\"classification/empatheticdialogues/test.csv\"\n",
    "\n",
    "# Download the dataset\n",
    "!wget 'https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz'\n",
    "# Extract the dataset to a directory\n",
    "!mkdir classification_data\n",
    "!tar -xvf empatheticdialogues.tar.gz -C classification_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool we see all our files. Let's poke at one of them before we start parsing our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_id</th>\n",
       "      <th>utterance_idx</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>speaker_idx</th>\n",
       "      <th>utterance</th>\n",
       "      <th>selfeval</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>1</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>I remember going to see the fireworks with my ...</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>2</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>0</td>\n",
       "      <td>Was this a friend you were in love with_comma_...</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>3</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>This was a best friend. I miss her.</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>4</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>0</td>\n",
       "      <td>Where has she gone?</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>5</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>We no longer talk.</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76663</th>\n",
       "      <td>hit:12424_conv:24848</td>\n",
       "      <td>5</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I found some pictures of my grandma in the att...</td>\n",
       "      <td>389</td>\n",
       "      <td>Yeah reminds me of the good old days.  I miss ...</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76664</th>\n",
       "      <td>hit:12424_conv:24849</td>\n",
       "      <td>1</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>294</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76665</th>\n",
       "      <td>hit:12424_conv:24849</td>\n",
       "      <td>2</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>389</td>\n",
       "      <td>Oh hey that's awesome!  That is awesome right?</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76666</th>\n",
       "      <td>hit:12424_conv:24849</td>\n",
       "      <td>3</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>294</td>\n",
       "      <td>It is soooo awesome.  We have been wanting a b...</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76667</th>\n",
       "      <td>hit:12424_conv:24849</td>\n",
       "      <td>4</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>389</td>\n",
       "      <td>That is awesome!!!! Congratulations!</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76668 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    conv_id  utterance_idx      context  \\\n",
       "0              hit:0_conv:1              1  sentimental   \n",
       "1              hit:0_conv:1              2  sentimental   \n",
       "2              hit:0_conv:1              3  sentimental   \n",
       "3              hit:0_conv:1              4  sentimental   \n",
       "4              hit:0_conv:1              5  sentimental   \n",
       "...                     ...            ...          ...   \n",
       "76663  hit:12424_conv:24848              5  sentimental   \n",
       "76664  hit:12424_conv:24849              1    surprised   \n",
       "76665  hit:12424_conv:24849              2    surprised   \n",
       "76666  hit:12424_conv:24849              3    surprised   \n",
       "76667  hit:12424_conv:24849              4    surprised   \n",
       "\n",
       "                                                  prompt  speaker_idx  \\\n",
       "0      I remember going to the fireworks with my best...            1   \n",
       "1      I remember going to the fireworks with my best...            0   \n",
       "2      I remember going to the fireworks with my best...            1   \n",
       "3      I remember going to the fireworks with my best...            0   \n",
       "4      I remember going to the fireworks with my best...            1   \n",
       "...                                                  ...          ...   \n",
       "76663  I found some pictures of my grandma in the att...          389   \n",
       "76664  I woke up this morning to my wife telling me s...          294   \n",
       "76665  I woke up this morning to my wife telling me s...          389   \n",
       "76666  I woke up this morning to my wife telling me s...          294   \n",
       "76667  I woke up this morning to my wife telling me s...          389   \n",
       "\n",
       "                                               utterance     selfeval tags  \n",
       "0      I remember going to see the fireworks with my ...  5|5|5_2|2|5  NaN  \n",
       "1      Was this a friend you were in love with_comma_...  5|5|5_2|2|5  NaN  \n",
       "2                    This was a best friend. I miss her.  5|5|5_2|2|5  NaN  \n",
       "3                                    Where has she gone?  5|5|5_2|2|5  NaN  \n",
       "4                                     We no longer talk.  5|5|5_2|2|5  NaN  \n",
       "...                                                  ...          ...  ...  \n",
       "76663  Yeah reminds me of the good old days.  I miss ...  5|5|5_5|5|5  NaN  \n",
       "76664  I woke up this morning to my wife telling me s...  5|5|5_5|5|5  NaN  \n",
       "76665     Oh hey that's awesome!  That is awesome right?  5|5|5_5|5|5  NaN  \n",
       "76666  It is soooo awesome.  We have been wanting a b...  5|5|5_5|5|5  NaN  \n",
       "76667               That is awesome!!!! Congratulations!  5|5|5_5|5|5  NaN  \n",
       "\n",
       "[76668 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the parse_dataset function below for short explanation.\n",
    "df = pd.read_csv(TRAIN_FILE, on_bad_lines='skip')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns we care about are:\n",
    "1. \"context\": This is the emotion we're trying to predict\n",
    "1. \"prompt\" and \"utterance\": We'll combine these sentences and use them as input \n",
    "\n",
    "Let's create a label encoder which converts our text labels to integer ids or vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
