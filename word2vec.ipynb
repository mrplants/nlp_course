{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vector Exploration\n",
    "In this notebook, we will explore word vectors from Spacy's `en_core_web_sm` model.  We will first\n",
    "load these word vectors into a vector database and then use the database to find to top-k most\n",
    "similar words.\n",
    "\n",
    "Next, we will explore analogies using the word vectors, an interesting application that is often\n",
    "used to evaluate the quality of word vectors and showcase biases in the data.\n",
    "\n",
    "Finally, we will use the word vectors to cluster words and visualize the clusters using t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import time\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between banana and fruit: 0.6650428369389225\n",
      "Similarity between banana and table: 0.20632129046587597\n"
     ]
    }
   ],
   "source": [
    "banana = nlp(\"banana\")\n",
    "fruit = nlp(\"fruit\")\n",
    "table = nlp(\"table\")\n",
    "print(f'Similarity between banana and fruit: {banana.similarity(fruit)}')\n",
    "print(f'Similarity between banana and table: {banana.similarity(table)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading all words: 100%|██████████| 776469/776469 [00:05<00:00, 132611.37it/s]\n",
      "Loading all vectors: 100%|██████████| 514092/514092 [00:00<00:00, 560132.66it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_faiss_index(nlp):\n",
    "    \"\"\"\n",
    "    Create a FAISS index for the word vectors in the given spaCy model and store words as metadata.\n",
    "\n",
    "    Args:\n",
    "        nlp: The spaCy language model.\n",
    "\n",
    "    Returns:\n",
    "        faiss_index: The FAISS index of word vectors.\n",
    "        word_list: List of words corresponding to the vectors in the FAISS index.\n",
    "    \"\"\"\n",
    "    # Filter the words to only those that have vectors\n",
    "    words = [word for word in tqdm(nlp.vocab.strings, \"Loading all words\") if nlp.vocab.has_vector(word)]\n",
    "\n",
    "    # Create a matrix to hold all vectors of words that have one\n",
    "    vectors = np.zeros((len(words), nlp.vocab.vectors_length), dtype='float32')\n",
    "    \n",
    "    for i, word in enumerate(tqdm(words, \"Loading all vectors\")):\n",
    "        vectors[i] = nlp.vocab.get_vector(word)\n",
    "    \n",
    "    # Normalize the vectors (important for cosine similarity)\n",
    "    faiss.normalize_L2(vectors)\n",
    "    \n",
    "    # Create a flat, L2 FAISS index\n",
    "    index = faiss.IndexFlatL2(vectors.shape[1])\n",
    "    index.add(vectors)\n",
    "    \n",
    "    return index, words\n",
    "\n",
    "# Create the FAISS index and word list\n",
    "faiss_index, words = create_faiss_index(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(faiss_index, words, query, nlp, topn=10):\n",
    "    \"\"\"\n",
    "    Find the most similar words to the query word using a FAISS index.\n",
    "\n",
    "    Args:\n",
    "        faiss_index: The FAISS index where word vectors are stored.\n",
    "        words: The list of words that corresponds to the vectors in the FAISS index.\n",
    "        query: The query word to find similarities for.\n",
    "        nlp: The loaded spaCy language model.\n",
    "        topn: The number of similar words to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of the most similar words with their similarity scores.\n",
    "    \"\"\"\n",
    "    # Get the vector for the query word\n",
    "    query_vector = nlp.vocab.get_vector(query).reshape(1, -1)\n",
    "    \n",
    "    # Normalize the query vector\n",
    "    faiss.normalize_L2(query_vector)\n",
    "    \n",
    "    # Search the index for similar vectors\n",
    "    distances, indices = faiss_index.search(query_vector, topn)\n",
    "    \n",
    "    # Return the words corresponding to the indices\n",
    "    return [(words[idx], 1 - distances[0][i]) for i, idx in enumerate(indices[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words most similar to \"dog\":\n",
      "1: bichon (1.00)\n",
      "2: dog (1.00)\n",
      "3: dog- (1.00)\n",
      "4: dogsbody (1.00)\n",
      "5: mastiff (1.00)\n",
      "6: Hotdogs (0.67)\n",
      "7: Muckdogs (0.67)\n",
      "8: Rottweilers (0.67)\n",
      "9: Saltdogs (0.67)\n",
      "10: bloodhounds (0.67)\n"
     ]
    }
   ],
   "source": [
    "# Find similar words\n",
    "similar_words = find_similar_words(faiss_index, words, 'dog', nlp, topn=10)\n",
    "print('Top 10 words most similar to \"dog\":')\n",
    "for index, (word, score) in enumerate(similar_words):\n",
    "    print(f'{index+1}: {word} ({score:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words most similar to \"cheese\":\n",
      "1: -St (1.00)\n",
      "2: Croquette (1.00)\n",
      "3: Gruyere (1.00)\n",
      "4: Gruyère (1.00)\n",
      "5: Parcheesi (1.00)\n",
      "6: Parmesan (1.00)\n",
      "7: bizzare (1.00)\n",
      "8: brie (1.00)\n",
      "9: cheddar (1.00)\n",
      "10: cheddars (1.00)\n"
     ]
    }
   ],
   "source": [
    "# Find similar words\n",
    "similar_words = find_similar_words(faiss_index, words, 'cheese', nlp, topn=10)\n",
    "print('Top 10 words most similar to \"cheese\":')\n",
    "for index, (word, score) in enumerate(similar_words):\n",
    "    print(f'{index+1}: {word} ({score:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words most similar to \"baseball\":\n",
      "1: Cueball (1.00)\n",
      "2: Curveball (1.00)\n",
      "3: Dodgeball (1.00)\n",
      "4: Eyeball (1.00)\n",
      "5: Hosenball (1.00)\n",
      "6: Knuckleball (1.00)\n",
      "7: Pokeball (1.00)\n",
      "8: Tomball (1.00)\n",
      "9: baseball (1.00)\n",
      "10: baseball-almanac.com (1.00)\n"
     ]
    }
   ],
   "source": [
    "# Find similar words\n",
    "similar_words = find_similar_words(faiss_index, words, 'baseball', nlp, topn=10)\n",
    "print('Top 10 words most similar to \"baseball\":')\n",
    "for index, (word, score) in enumerate(similar_words):\n",
    "    print(f'{index+1}: {word} ({score:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.1401   4.0082  -1.2984  ...  1.9019  -0.95315 -0.49466]\n",
      " [-3.1401   4.0082  -1.2984  ...  1.9019  -0.95315 -0.49466]\n",
      " [-3.1401   4.0082  -1.2984  ...  1.9019  -0.95315 -0.49466]\n",
      " ...\n",
      " [-3.1401   4.0082  -1.2984  ...  1.9019  -0.95315 -0.49466]\n",
      " [-3.1401   4.0082  -1.2984  ...  1.9019  -0.95315 -0.49466]\n",
      " [-3.1401   4.0082  -1.2984  ...  1.9019  -0.95315 -0.49466]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "perplexity must be less than n_samples",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Replace 'your_list_of_words' with the actual words you want to visualize\u001b[39;00m\n\u001b[1;32m     47\u001b[0m your_list_of_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word, _ \u001b[38;5;129;01min\u001b[39;00m find_similar_words(faiss_index, words, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseball\u001b[39m\u001b[38;5;124m'\u001b[39m, nlp, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)]\n\u001b[0;32m---> 48\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mplot_3d_tsne_for_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43myour_list_of_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m fig\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[17], line 26\u001b[0m, in \u001b[0;36mplot_3d_tsne_for_words\u001b[0;34m(word_list)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Run t-SNE\u001b[39;00m\n\u001b[1;32m     25\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m vectors_reduced \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for the t-SNE results\u001b[39;00m\n\u001b[1;32m     29\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(vectors_reduced, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Developer/nlp_course/nlp_course_env/lib/python3.11/site-packages/sklearn/utils/_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    279\u001b[0m         )\n",
      "File \u001b[0;32m~/Developer/nlp_course/nlp_course_env/lib/python3.11/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/nlp_course/nlp_course_env/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:1125\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;66;03m# TSNE.metric is not validated yet\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m \n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;124;03m        Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1125\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n",
      "File \u001b[0;32m~/Developer/nlp_course/nlp_course_env/lib/python3.11/site-packages/sklearn/manifold/_t_sne.py:836\u001b[0m, in \u001b[0;36mTSNE._check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 836\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity must be less than n_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: perplexity must be less than n_samples"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "def plot_3d_tsne_for_words(word_list):\n",
    "    \"\"\"\n",
    "    Create a 3D t-SNE visualization for the specified list of words using spaCy embeddings.\n",
    "\n",
    "    Args:\n",
    "        word_list (list of str): A list of words to visualize.\n",
    "\n",
    "    Returns:\n",
    "        A plotly figure object that can be displayed in a Jupyter notebook.\n",
    "    \"\"\"\n",
    "    # Load the spaCy model\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    \n",
    "    # Get vectors for the words\n",
    "    vectors = np.array([nlp.vocab.get_vector(word) for word in word_list if nlp.vocab.has_vector(word)])\n",
    "    print(vectors)\n",
    "    \n",
    "    # Run t-SNE\n",
    "    tsne = TSNE(n_components=3, random_state=42)\n",
    "    vectors_reduced = tsne.fit_transform(vectors)\n",
    "    \n",
    "    # Create a DataFrame for the t-SNE results\n",
    "    df = pd.DataFrame(vectors_reduced, columns=['x', 'y', 'z'])\n",
    "    df['word'] = [word for word in word_list if nlp.vocab.has_vector(word)]\n",
    "    \n",
    "    # Create a 3D scatter plot\n",
    "    fig = px.scatter_3d(df, x='x', y='y', z='z', text='word', hover_data=['word'])\n",
    "    \n",
    "    # Improve layout\n",
    "    fig.update_traces(textposition='top center')\n",
    "    fig.update_layout(title=\"3D t-SNE Word Vectors\",\n",
    "                      scene=dict(xaxis_title='t-SNE dimension 1',\n",
    "                                 yaxis_title='t-SNE dimension 2',\n",
    "                                 zaxis_title='t-SNE dimension 3'),\n",
    "                      margin=dict(l=0, r=0, b=0, t=0))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'your_list_of_words' with the actual words you want to visualize\n",
    "your_list_of_words = [word for word, _ in find_similar_words(faiss_index, words, 'baseball', nlp, topn=20)]\n",
    "fig = plot_3d_tsne_for_words(your_list_of_words)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
